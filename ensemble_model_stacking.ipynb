{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:47:52.902793Z",
     "start_time": "2024-05-23T12:47:52.892652Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test Dataset formating"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:11:09.667079Z",
     "start_time": "2024-05-23T13:11:09.655011Z"
    }
   },
   "cell_type": "code",
   "source": "final_test_df = pd.read_csv('data/Sample_sub.csv')",
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:11:10.748923Z",
     "start_time": "2024-05-23T13:11:10.744890Z"
    }
   },
   "source": "test_df = pd.read_csv('data/Sample_sub.csv')",
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T14:06:27.250746Z",
     "start_time": "2024-05-23T14:06:27.196059Z"
    }
   },
   "source": "test_df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      capture_site  week_caught  turtles_rescued\n",
       "0                0            1                7\n",
       "1                0            2                1\n",
       "2                0            3                5\n",
       "3                0            4                2\n",
       "4                0            5                3\n",
       "...            ...          ...              ...\n",
       "1271             9           40                0\n",
       "1272             9           41                7\n",
       "1273             9           42                7\n",
       "1274             9           43                3\n",
       "1275             9           44                5\n",
       "\n",
       "[1276 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capture_site</th>\n",
       "      <th>week_caught</th>\n",
       "      <th>turtles_rescued</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>9</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>9</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>9</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1276 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:48:04.106946Z",
     "start_time": "2024-05-23T12:48:04.104448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_capture_site(df):\n",
    "    df['capture_site'] = df['ID'].apply(lambda x: x.split('_')[-2])\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:48:04.665947Z",
     "start_time": "2024-05-23T12:48:04.663677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_and_convert_week(df):\n",
    "    # Extract the second to last element\n",
    "    df['week_caught'] = df['ID'].apply(lambda x: x[-2:])\n",
    "\n",
    "    # Convert to datetime with appropriate format for year and month (\"%Y%m\")\n",
    "    df['week_caught'] = df['week_caught'].apply(lambda x : int(x))\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:48:07.105578Z",
     "start_time": "2024-05-23T12:48:07.093703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def formating_sample_sub(df): \n",
    "    # extracting capture site id\n",
    "    df = extract_capture_site(df)\n",
    "    \n",
    "    # extracting week of rascue\n",
    "    df = extract_and_convert_week(df)\n",
    "    \n",
    "    # renaming columns to match training set\n",
    "    df.rename(columns={'Capture_Number': 'turtles_rescued'}, inplace=True)\n",
    "    \n",
    "    # getting rid of mixed column\n",
    "    df.drop(columns=['ID'], inplace=True)\n",
    "    \n",
    "    # Standartising prediction \n",
    "    df = df.groupby(['capture_site', 'week_caught'])['turtles_rescued'].sum().reset_index()\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:11:18.216634Z",
     "start_time": "2024-05-23T13:11:18.169467Z"
    }
   },
   "cell_type": "code",
   "source": "test_df = formating_sample_sub(test_df)",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:11:18.940298Z",
     "start_time": "2024-05-23T13:11:18.928355Z"
    }
   },
   "cell_type": "code",
   "source": "test_df.to_csv('data/test_df.csv', index=False)",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:11:32.443446Z",
     "start_time": "2024-05-23T13:11:32.418462Z"
    }
   },
   "cell_type": "code",
   "source": "test_df = pd.read_csv('data/test_df.csv')",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train Dataset Formating"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:57:58.277872Z",
     "start_time": "2024-05-23T12:57:58.204340Z"
    }
   },
   "source": [
    "train_df = pd.read_csv('data/train.csv')"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "train_df.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:58:10.832404Z",
     "start_time": "2024-05-23T12:58:10.826041Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "# Defining function to Standartising column names \n",
    "def standardize_column_names(col):\n",
    "    # Replace spaces with underscores\n",
    "    col = col.replace(' ', '_')\n",
    "    # Insert underscore before each uppercase letter preceded by a lowercase letter or followed by a lowercase letter\n",
    "    col = re.sub(r'(?<=[a-z])(?=[A-Z])', '_', col)\n",
    "    col = re.sub(r'(?<=[A-Z])(?=[A-Z][a-z])', '_', col)\n",
    "    # Convert to lower case\n",
    "    col = col.lower()\n",
    "    # Ensure single underscores only (in case of consecutive underscores from initial spaces)\n",
    "    col = re.sub(r'_+', '_', col)\n",
    "    return col\n"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:58:12.324296Z",
     "start_time": "2024-05-23T12:58:12.321467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Applying function to df\n",
    "train_df.columns = [standardize_column_names(col) for col in train_df.columns]\n",
    "\n",
    "# Printing the updated column names to verify the changes\n",
    "print(train_df.columns)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['rescue_id', 'date_time_caught', 'researcher', 'capture_site',\n",
      "       'foraging_ground', 'capture_method', 'fisher', 'landing_site',\n",
      "       'species', 'tag_1', 'tag_2', 'lost_tags', 't_number', 'ccl_cm',\n",
      "       'ccw_cm', 'weight_kg', 'sex', 'turtle_characteristics', 'status',\n",
      "       'release_site', 'date_time_release'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:55:45.082727Z",
     "start_time": "2024-05-23T12:55:45.080154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the function to extract the number\n",
    "def extract_number_split(s):\n",
    "    return int(s.split('_')[-1])\n",
    "\n",
    "# Define a function to apply the extraction to multiple columns\n",
    "def apply_extraction(df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(extract_number_split)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:58:16.337493Z",
     "start_time": "2024-05-23T12:58:16.307578Z"
    }
   },
   "source": [
    "columns_to_extract_train = ['fisher', 'researcher', 'capture_site', 'species']\n",
    "train_df = apply_extraction(train_df, columns_to_extract_train)"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:58:19.510717Z",
     "start_time": "2024-05-23T12:58:19.503959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columns_to_drop = ['rescue_id', 'turtle_characteristics', 'tag_1', 'tag_2', 'lost_tags', 't_number', 'sex',\n",
    "                   'capture_method', 'release_site', 'landing_site', 'status', 'foraging_ground', 'date_time_release']\n",
    "\n",
    "train_df = train_df.drop(columns=columns_to_drop)"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:03:55.163923Z",
     "start_time": "2024-05-23T13:03:55.160113Z"
    }
   },
   "source": [
    "def convert_and_split_datetime(df, columns):\n",
    "    \"\"\"\n",
    "    Convert specified datetime columns to timestamp and split into year and week columns\n",
    "    with new names based on the original column names.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the columns.\n",
    "    columns (list): List of column names to convert and split.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with new year and week columns.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        # Convert the column to datetime\n",
    "        df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "\n",
    "        # Extract the base name without 'date_time_' prefix\n",
    "        base_name = column.replace('date_time_', '')\n",
    "\n",
    "        # Create new columns for year and week with the desired names\n",
    "        df[f'year_{base_name}'] = df[column].dt.year\n",
    "        df[f'week_{base_name}'] = df[column].dt.isocalendar().week\n",
    "\n",
    "        # Drop the original datetime column if desired\n",
    "        df.drop(columns=[column], inplace=True)\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:03:57.159932Z",
     "start_time": "2024-05-23T13:03:57.145438Z"
    }
   },
   "source": [
    "# Apply function to train_df\n",
    "columns_to_convert = ['date_time_caught']\n",
    "train_df = convert_and_split_datetime(train_df, columns_to_convert)\n",
    "\n",
    "train_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   researcher  capture_site  fisher  species  ccl_cm  ccw_cm  weight_kg   \n",
       "0          25             0    1072        6   64.70   62.60        NaN  \\\n",
       "1           6             0     520        6   35.85   31.35        NaN   \n",
       "2           6             0    1669        5   51.80   49.20        NaN   \n",
       "3          32             0    1798        6   60.50   59.00        NaN   \n",
       "4          25             0    1918        5   34.70   33.00        NaN   \n",
       "\n",
       "   year_caught  week_caught  \n",
       "0         2000           51  \n",
       "1         2001           43  \n",
       "2         2001           44  \n",
       "3         2002           11  \n",
       "4         2002           32  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>researcher</th>\n",
       "      <th>capture_site</th>\n",
       "      <th>fisher</th>\n",
       "      <th>species</th>\n",
       "      <th>ccl_cm</th>\n",
       "      <th>ccw_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>year_caught</th>\n",
       "      <th>week_caught</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1072</td>\n",
       "      <td>6</td>\n",
       "      <td>64.70</td>\n",
       "      <td>62.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>6</td>\n",
       "      <td>35.85</td>\n",
       "      <td>31.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1669</td>\n",
       "      <td>5</td>\n",
       "      <td>51.80</td>\n",
       "      <td>49.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>1798</td>\n",
       "      <td>6</td>\n",
       "      <td>60.50</td>\n",
       "      <td>59.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1918</td>\n",
       "      <td>5</td>\n",
       "      <td>34.70</td>\n",
       "      <td>33.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imputing Missing Data in Weight"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:07:56.564258Z",
     "start_time": "2024-05-23T13:07:56.559676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "def imput_missing_weight_values(df, n = 5):\n",
    "    knn_df = df[['ccl_cm', 'ccw_cm', 'weight_kg']]\n",
    "    imputer = KNNImputer(n_neighbors=n)\n",
    "    imputer.set_output(transform='pandas')\n",
    "\n",
    "    return imputer.fit_transform(knn_df)"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:08:02.060021Z",
     "start_time": "2024-05-23T13:07:57.390855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imputed_df = imput_missing_weight_values(train_df)\n",
    "train_df['ccl_cm'] = imputed_df['ccl_cm']\n",
    "train_df['ccw_cm'] = imputed_df['ccw_cm']\n",
    "train_df['weight_kg'] = imputed_df['weight_kg']"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:08:09.080822Z",
     "start_time": "2024-05-23T13:08:09.050519Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.info()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18062 entries, 0 to 18061\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   researcher    18062 non-null  int64  \n",
      " 1   capture_site  18062 non-null  int64  \n",
      " 2   fisher        18062 non-null  int64  \n",
      " 3   species       18062 non-null  int64  \n",
      " 4   ccl_cm        18062 non-null  float64\n",
      " 5   ccw_cm        18062 non-null  float64\n",
      " 6   weight_kg     18062 non-null  float64\n",
      " 7   year_caught   18062 non-null  int32  \n",
      " 8   week_caught   18062 non-null  UInt32 \n",
      "dtypes: UInt32(1), float64(3), int32(1), int64(4)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:10:03.419255Z",
     "start_time": "2024-05-23T13:10:03.346092Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.to_csv('data/train.csv')",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_df = pd.read_csv('data/train.csv')"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Baseline Model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:41:06.882947Z",
     "start_time": "2024-05-22T18:41:06.852022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_df = train_df.groupby(['year_caught', 'capture_site', 'week_caught']).size().reset_index(name='turtles_rescued')\n",
    "baseline_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      year_caught  capture_site  week_caught  turtles_rescued\n",
       "0            1998            11           28                1\n",
       "1            1998            11           32                1\n",
       "2            1998            11           39                2\n",
       "3            1998            11           43                1\n",
       "4            1998            11           45                1\n",
       "...           ...           ...          ...              ...\n",
       "7952         2018            27           36                1\n",
       "7953         2018            27           38                1\n",
       "7954         2018            27           45                1\n",
       "7955         2018            28           44                1\n",
       "7956         2018            28           45                1\n",
       "\n",
       "[7957 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_caught</th>\n",
       "      <th>capture_site</th>\n",
       "      <th>week_caught</th>\n",
       "      <th>turtles_rescued</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7952</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7953</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7954</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7955</th>\n",
       "      <td>2018</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7956</th>\n",
       "      <td>2018</td>\n",
       "      <td>28</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7957 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 317
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:41:10.588987Z",
     "start_time": "2024-05-22T18:41:10.577547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_df = baseline_df[~baseline_df['year_caught'].between(1988, 2006)].reset_index(drop=True)\n",
    "baseline_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      year_caught  capture_site  week_caught  turtles_rescued\n",
       "0            2007             2            2                1\n",
       "1            2007             2            3                2\n",
       "2            2007             2            4                1\n",
       "3            2007             2            5                1\n",
       "4            2007             2            7                1\n",
       "...           ...           ...          ...              ...\n",
       "6461         2018            27           36                1\n",
       "6462         2018            27           38                1\n",
       "6463         2018            27           45                1\n",
       "6464         2018            28           44                1\n",
       "6465         2018            28           45                1\n",
       "\n",
       "[6466 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_caught</th>\n",
       "      <th>capture_site</th>\n",
       "      <th>week_caught</th>\n",
       "      <th>turtles_rescued</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6461</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6462</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6463</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6464</th>\n",
       "      <td>2018</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6465</th>\n",
       "      <td>2018</td>\n",
       "      <td>28</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6466 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 318
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:41:13.162677Z",
     "start_time": "2024-05-22T18:41:13.157521Z"
    }
   },
   "cell_type": "code",
   "source": "baseline_df.drop(['year_caught'], axis=1, inplace=True)",
   "outputs": [],
   "execution_count": 319
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:24:52.449296Z",
     "start_time": "2024-05-23T13:24:52.442201Z"
    }
   },
   "cell_type": "code",
   "source": "baseline_test = test_df.copy()",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:33:03.312654Z",
     "start_time": "2024-05-23T10:33:03.261826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaselinePredictor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def predict_turtles_rescued_all(self):\n",
    "        # Initialize an empty list to store dataframes\n",
    "        dfs = []\n",
    "\n",
    "        # Get unique capture sites and weeks in the baseline_df\n",
    "        capture_site_all = self.df['capture_site'].unique()\n",
    "        weeks_in_year = self.df['week_caught'].unique()\n",
    "\n",
    "        # Iterate over each capture site and week to calculate the mean turtles_rescued\n",
    "        for capture_site in capture_site_all:\n",
    "            for week in weeks_in_year:\n",
    "                # Calculate mean turtles_rescued for the current capture site and week\n",
    "                mean_turtles_rescued = self.df[(self.df['capture_site'] == capture_site) & (self.df['week_caught'] == week)]['turtles_rescued'].mean()\n",
    "\n",
    "                # Append a dataframe to the list\n",
    "                dfs.append(pd.DataFrame({'capture_site': [capture_site], 'week_caught': [week], 'turtles_rescued': [mean_turtles_rescued]}))\n",
    "\n",
    "        # Concatenate all dataframes in the list\n",
    "        predict_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        return predict_df"
   ],
   "outputs": [],
   "execution_count": 329
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:33:10.299303Z",
     "start_time": "2024-05-23T10:33:09.409572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the predictor with the baseline_df\n",
    "predictor = BaselinePredictor(baseline_df)\n",
    "\n",
    "# Predict the baseline values\n",
    "predict_baseline = predictor.predict_turtles_rescued_all()\n",
    "\n",
    "# Print the predicted baseline DataFrame\n",
    "print(predict_baseline)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      capture_site  week_caught  turtles_rescued\n",
      "0                2            2         1.666667\n",
      "1                2            3         1.333333\n",
      "2                2            4         1.000000\n",
      "3                2            5         1.500000\n",
      "4                2            7         1.000000\n",
      "...            ...          ...              ...\n",
      "1532            17           37         2.333333\n",
      "1533            17           39         1.000000\n",
      "1534            17           33         1.250000\n",
      "1535            17           20         1.000000\n",
      "1536            17           53         1.000000\n",
      "\n",
      "[1537 rows x 3 columns]\n"
     ]
    }
   ],
   "execution_count": 330
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:44:15.808104Z",
     "start_time": "2024-05-23T10:44:15.765011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Ensure both DataFrames have the same number of rows\n",
    "num_rows_baseline_test = baseline_test.shape[0]\n",
    "\n",
    "# Randomly sample rows from predict_baseline to match the number of rows in sample_sub\n",
    "predict_baseline_trimmed = predict_baseline.sample(n=num_rows_baseline_test, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Ensure the indices match\n",
    "baseline_test = baseline_test.reset_index(drop=True)\n",
    "\n",
    "# Combine both DataFrames to ensure we drop NaNs in corresponding rows\n",
    "combined_df = pd.concat([baseline_test['turtles_rescued'], predict_baseline_trimmed['turtles_rescued']], axis=1, keys=['true', 'pred'])\n",
    "\n",
    "# Drop rows with NaN values in either column\n",
    "combined_df = combined_df.dropna()\n",
    "\n",
    "# Separate the true and predicted values\n",
    "y_true = combined_df['true']\n",
    "y_pred = combined_df['pred']\n",
    "\n",
    "# Calculate MAE\n",
    "mae_baseline = mean_absolute_error(y_true, y_pred)\n",
    "print(mae_baseline)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2108299243509903\n"
     ]
    }
   ],
   "execution_count": 340
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Contextualising MAE"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:33:44.626389Z",
     "start_time": "2024-05-23T10:33:44.610908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(baseline_test.turtles_rescued.min())\n",
    "print(baseline_test.turtles_rescued.max())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "9\n"
     ]
    }
   ],
   "execution_count": 331
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:33:49.338193Z",
     "start_time": "2024-05-23T10:33:49.315886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_min = 1\n",
    "target_max = 9\n",
    "target_range = target_max - target_min\n",
    "acceptable_mae = target_range * 0.1  # Example threshold of 10% of the range\n",
    "print(f\"Acceptable MAE: {acceptable_mae}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceptable MAE: 0.8\n"
     ]
    }
   ],
   "execution_count": 332
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RMSE Baseline"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:46:41.201395Z",
     "start_time": "2024-05-23T10:46:41.170267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(sample_sub['turtles_rescued'].describe())\n",
    "\n",
    "# Baseline RMSE\n",
    "mean_turtles_rescued = baseline_test['turtles_rescued'].mean()\n",
    "baseline_predictions = [mean_turtles_rescued] * len(sample_sub)\n",
    "baseline_mse = mean_squared_error(baseline_test['turtles_rescued'], baseline_predictions)\n",
    "baseline_rmse = np.sqrt(baseline_mse)\n",
    "print(f\"Baseline RMSE: {baseline_rmse}\")\n",
    "\n",
    "# Coefficient of Variation of RMSE\n",
    "cv_rmse = (rmse / mean_turtles_rescued) * 100\n",
    "print(f\"Coefficient of Variation of RMSE: {cv_rmse:.2f}%\")\n",
    "\n",
    "# Standard Deviation of Turtles Rescued\n",
    "std_turtles_rescued = baseline_test['turtles_rescued'].std()\n",
    "print(f\"Standard Deviation of Turtles Rescued: {std_turtles_rescued}\")\n",
    "print(f\"RMSE as a proportion of Standard Deviation: {rmse / std_turtles_rescued}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 3.9440045387035063\n",
      "count    1276.000000\n",
      "mean        4.436520\n",
      "std         2.878706\n",
      "min         0.000000\n",
      "25%         2.000000\n",
      "50%         4.000000\n",
      "75%         7.000000\n",
      "max         9.000000\n",
      "Name: turtles_rescued, dtype: float64\n",
      "Baseline RMSE: 2.8775776437795377\n",
      "Coefficient of Variation of RMSE: 88.90%\n",
      "Standard Deviation of Turtles Rescued: 2.878705884420333\n",
      "RMSE as a proportion of Standard Deviation: 1.370061651677794\n"
     ]
    }
   ],
   "execution_count": 341
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Benchmarking against baseline"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:46:55.825671Z",
     "start_time": "2024-05-23T10:46:55.799557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the MAE for the baseline model\n",
    "baseline_mae = mean_absolute_error(sample_sub['turtles_rescued'], baseline_predictions)\n",
    "print(f\"Baseline MAE: {baseline_mae}\")\n",
    "\n",
    "# Compare baseline MAE with your model's MAE\n",
    "if mae_baseline < baseline_mae:\n",
    "    print(\"Your model is performing better than the baseline.\")\n",
    "else:\n",
    "    print(\"Your model is not performing better than the baseline.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE: 2.519082703589784\n",
      "Your model is not performing better than the baseline.\n"
     ]
    }
   ],
   "execution_count": 342
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Interactive version"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T14:08:17.766516Z",
     "start_time": "2024-05-23T14:08:17.756153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_turtles_rescued(capture_site, week):\n",
    "    predict_df = baseline_df[(baseline_df['capture_site'] == 5) & (baseline_df['week_caught'] == 5)]\n",
    "    turtle_rescued = predict_df['turtles_rescued'].mean()\n",
    "    return turtle_rescued"
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T14:08:19.974139Z",
     "start_time": "2024-05-23T14:08:19.923355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "week = input(f'Enter a week number for which you would like to predict (from 1 to {len(baseline_df.week_caught.unique())}):')\n",
    "capture_site = input(f'Enter a capture site number (from 1 to 29):')\n",
    "print(f'Predicted Turtles Rescued for {capture_site} and {week}: {predict_turtles_rescued(capture_site, week)}')"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[88], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m week \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEnter a week number for which you would like to predict (from 1 to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(\u001B[43mbaseline_df\u001B[49m\u001B[38;5;241m.\u001B[39mweek_caught\u001B[38;5;241m.\u001B[39munique())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m):\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m capture_site \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEnter a capture site number (from 1 to 29):\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPredicted Turtles Rescued for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcapture_site\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mweek\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpredict_turtles_rescued(capture_site,\u001B[38;5;250m \u001B[39mweek)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'baseline_df' is not defined"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Enseble Model Staking"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pipeline building"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T16:43:14.130703Z",
     "start_time": "2024-05-23T16:43:13.939265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# machine-learning stack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    PolynomialFeatures,\n",
    "    FunctionTransformer\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imb_pipe"
   ],
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predicting what turtles are caught by week"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# hypothesis is that turtles are effected (due to species, weight, etc) by week of the year (e.g seasons)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:47:43.819088Z",
     "start_time": "2024-05-23T13:47:43.814795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# loading data for model\n",
    "week_model_df = train_df.copy()"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:47:47.428692Z",
     "start_time": "2024-05-23T13:47:47.395647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 30% examples in test data\n",
    "train_week, test_week = train_test_split(week_model_df, test_size = 0.3, random_state=1) "
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:48:38.271748Z",
     "start_time": "2024-05-23T13:48:38.260147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = train_week.drop('week_caught', axis=1, inplace=True) # capture site? \n",
    "y = test_week.copy()"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:49:12.021117Z",
     "start_time": "2024-05-23T13:49:12.015594Z"
    }
   },
   "cell_type": "code",
   "source": "x = train_week.copy()",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T14:10:19.631575Z",
     "start_time": "2024-05-23T14:10:19.608692Z"
    }
   },
   "cell_type": "code",
   "source": "x",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       researcher  capture_site  fisher  species  ccl_cm  ccw_cm  weight_kg   \n",
       "4586           20            15    1473        6   50.00    48.3     15.590  \\\n",
       "6898           20            19    1448        6   44.37    41.2     10.480   \n",
       "17127          20             9    1230        6   41.10    38.1      7.400   \n",
       "5417           30            16    1478        5   38.60    37.7      7.386   \n",
       "17261          20             9     996        6   28.90    28.5      3.180   \n",
       "...           ...           ...     ...      ...     ...     ...        ...   \n",
       "10955          30            25    1478        5   86.50    76.1     78.470   \n",
       "17289          30             9    1415        6   46.00    41.5     11.700   \n",
       "5192           20            15    1343        6   32.30    30.0      3.500   \n",
       "12172          20            25    1464        5   37.30    37.1      6.500   \n",
       "235            20             1     818        5   37.20    37.2      6.500   \n",
       "\n",
       "       year_caught  \n",
       "4586          2016  \n",
       "6898          2018  \n",
       "17127         2015  \n",
       "5417          2005  \n",
       "17261         2015  \n",
       "...            ...  \n",
       "10955         2005  \n",
       "17289         2015  \n",
       "5192          2018  \n",
       "12172         2011  \n",
       "235           2010  \n",
       "\n",
       "[12643 rows x 8 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>researcher</th>\n",
       "      <th>capture_site</th>\n",
       "      <th>fisher</th>\n",
       "      <th>species</th>\n",
       "      <th>ccl_cm</th>\n",
       "      <th>ccw_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>year_caught</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4586</th>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>1473</td>\n",
       "      <td>6</td>\n",
       "      <td>50.00</td>\n",
       "      <td>48.3</td>\n",
       "      <td>15.590</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6898</th>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>1448</td>\n",
       "      <td>6</td>\n",
       "      <td>44.37</td>\n",
       "      <td>41.2</td>\n",
       "      <td>10.480</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17127</th>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>1230</td>\n",
       "      <td>6</td>\n",
       "      <td>41.10</td>\n",
       "      <td>38.1</td>\n",
       "      <td>7.400</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5417</th>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>1478</td>\n",
       "      <td>5</td>\n",
       "      <td>38.60</td>\n",
       "      <td>37.7</td>\n",
       "      <td>7.386</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261</th>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>996</td>\n",
       "      <td>6</td>\n",
       "      <td>28.90</td>\n",
       "      <td>28.5</td>\n",
       "      <td>3.180</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>1478</td>\n",
       "      <td>5</td>\n",
       "      <td>86.50</td>\n",
       "      <td>76.1</td>\n",
       "      <td>78.470</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17289</th>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>1415</td>\n",
       "      <td>6</td>\n",
       "      <td>46.00</td>\n",
       "      <td>41.5</td>\n",
       "      <td>11.700</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>1343</td>\n",
       "      <td>6</td>\n",
       "      <td>32.30</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.500</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12172</th>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>1464</td>\n",
       "      <td>5</td>\n",
       "      <td>37.30</td>\n",
       "      <td>37.1</td>\n",
       "      <td>6.500</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>818</td>\n",
       "      <td>5</td>\n",
       "      <td>37.20</td>\n",
       "      <td>37.2</td>\n",
       "      <td>6.500</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12643 rows × 8 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T13:46:22.075259Z",
     "start_time": "2024-05-23T13:46:21.978312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prior to training our model, we’ll set aside a portion of our data in order to evaluate its performance.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=1)"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[69], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Prior to training our model, we’ll set aside a portion of our data in order to evaluate its performance.\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2561\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[1;32m   2557\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt least one array required as input\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2559\u001B[0m arrays \u001B[38;5;241m=\u001B[39m indexable(\u001B[38;5;241m*\u001B[39marrays)\n\u001B[0;32m-> 2561\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m \u001B[43m_num_samples\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2562\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m _validate_shuffle_split(\n\u001B[1;32m   2563\u001B[0m     n_samples, test_size, train_size, default_test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m\n\u001B[1;32m   2564\u001B[0m )\n\u001B[1;32m   2566\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m shuffle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/sklearn/utils/validation.py:331\u001B[0m, in \u001B[0;36m_num_samples\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    329\u001B[0m         x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(x)\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 331\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(message)\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m x\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    334\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[0;31mTypeError\u001B[0m: Expected sequence or array-like, got <class 'NoneType'>"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predicting what turtles are caught by capture_site"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transforming "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T18:47:27.800474Z",
     "start_time": "2024-05-23T18:47:27.752823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform_df = train_df.groupby(['year_caught', 'capture_site', 'week_caught']).size().reset_index(name='turtles_rescued')\n",
    "transform_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      year_caught  capture_site  week_caught  turtles_rescued\n",
       "0            1998            11           28                1\n",
       "1            1998            11           32                1\n",
       "2            1998            11           39                2\n",
       "3            1998            11           43                1\n",
       "4            1998            11           45                1\n",
       "...           ...           ...          ...              ...\n",
       "7952         2018            27           36                1\n",
       "7953         2018            27           38                1\n",
       "7954         2018            27           45                1\n",
       "7955         2018            28           44                1\n",
       "7956         2018            28           45                1\n",
       "\n",
       "[7957 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_caught</th>\n",
       "      <th>capture_site</th>\n",
       "      <th>week_caught</th>\n",
       "      <th>turtles_rescued</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7952</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7953</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7954</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7955</th>\n",
       "      <td>2018</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7956</th>\n",
       "      <td>2018</td>\n",
       "      <td>28</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7957 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T19:15:52.966302Z",
     "start_time": "2024-05-23T19:15:52.957866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class TurtleRescueModifierTrain:\n",
    "    def __init__(self):\n",
    "        # Generate arrays for year, week_caught, and capture_site\n",
    "        years = np.repeat(np.arange(1988, 2019), 53)\n",
    "        week_caught = np.tile(np.arange(1, 54), 31)\n",
    "        capture_site = np.repeat(np.arange(1, 32), 53)\n",
    "\n",
    "        # Create the DataFrame\n",
    "        self.target_df = pd.DataFrame({\n",
    "            'year_caught': years,\n",
    "            'week_caught': week_caught,\n",
    "            'capture_site': capture_site,\n",
    "            'turtles_rescued': np.zeros(len(years))\n",
    "        })\n",
    "\n",
    "    def merge_data(self, source_df):\n",
    "        for index, row in source_df.iterrows():\n",
    "            # Match conditions based on year_caught, week_caught, and capture_site\n",
    "            match_condition = (\n",
    "                    (self.target_df['year_caught'] == row['year_caught']) &\n",
    "                    (self.target_df['week_caught'] == row['week_caught']) &\n",
    "                    (self.target_df['capture_site'] == row['capture_site'])\n",
    "            )\n",
    "\n",
    "            # Check if a matching row exists in the target_df\n",
    "            matching_row_index = self.target_df.index[match_condition]\n",
    "\n",
    "            if len(matching_row_index) > 0:\n",
    "                # Update the existing row in target_df with data from source_df\n",
    "                self.target_df.loc[matching_row_index[0], 'turtles_rescued'] = row['turtles_rescued']\n",
    "            else:\n",
    "                # If no matching row exists, create a new row in target_df\n",
    "                new_row = {\n",
    "                    'year_caught': row['year_caught'],\n",
    "                    'week_caught': row['week_caught'],\n",
    "                    'capture_site': row['capture_site'],\n",
    "                    'turtles_rescued': row['turtles_rescued']\n",
    "                }\n",
    "                self.target_df = self.target_df.append(new_row, ignore_index=True)\n",
    "\n",
    "        return self.target_df\n",
    "    def impute_missing_values(self, transform_df):\n",
    "        \"\"\"\n",
    "        Impute missing values in self.df using K-Nearest Neighbors (KNN) based on transform_df.\n",
    "        \"\"\"\n",
    "        # Initialize KNN Imputer with k=5\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "        # Iterate through rows of transform_df\n",
    "        for index, row in transform_df.iterrows():\n",
    "            # Extract features for imputation\n",
    "            features = self.df[\n",
    "                (self.df['year_caught'] == row['year_caught']) &\n",
    "                (self.df['week_caught'] == row['week_caught']) &\n",
    "                (self.df['capture_site'] == row['capture_site'])\n",
    "                ][['year_caught', 'week_caught', 'capture_site']]\n",
    "\n",
    "            # Check if any missing values need to be imputed\n",
    "            if features.empty:\n",
    "                # Impute missing values for the current row of transform_df\n",
    "                features = [[row['year_caught'], row['week_caught'], row['capture_site']]]\n",
    "                imputed_values = imputer.fit_transform(features)\n",
    "                self.df.loc[len(self.df)] = np.append(imputed_values[0], row['turtles_rescued'])\n",
    "\n",
    "        return self.df\n"
   ],
   "outputs": [],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T19:15:54.605814Z",
     "start_time": "2024-05-23T19:15:54.569078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transformed_train = TurtleRescueModifierTrain().merge_data(transform_df)\n",
    "transformed_train"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/sx/vzszgjy10_z93jmql4144jnm0000gn/T/ipykernel_18567/1849899029.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtransformed_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTurtleRescueModifierTrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmerge_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtransform_df\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mtransformed_train\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/sx/vzszgjy10_z93jmql4144jnm0000gn/T/ipykernel_18567/4291845236.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, source_df)\u001B[0m\n\u001B[1;32m     39\u001B[0m                     \u001B[0;34m'week_caught'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'week_caught'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m                     \u001B[0;34m'capture_site'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'capture_site'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m                     \u001B[0;34m'turtles_rescued'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'turtles_rescued'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m                 }\n\u001B[0;32m---> 43\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_row\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget_df\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   5985\u001B[0m             \u001B[0;32mand\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_accessors\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5986\u001B[0m             \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_info_axis\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_can_hold_identifiers_and_holds_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5987\u001B[0m         ):\n\u001B[1;32m   5988\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5989\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mobject\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getattribute__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T19:10:23.562629Z",
     "start_time": "2024-05-23T19:10:23.552269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TurtleRescueContainer:\n",
    "    def __init__(self):\n",
    "        # Initialize the DataFrame with 53 weeks and 29 capture sites\n",
    "        self.df = pd.DataFrame({\n",
    "            'week_caught': np.tile(np.arange(1, 54), 29),\n",
    "            'capture_site': np.repeat(np.arange(1, 30), 53),\n",
    "            'turtles_rescued': np.zeros(53 * 29),\n",
    "            'weight_week': np.zeros(53 * 29),\n",
    "            'weight_capture_site': np.zeros(53 * 29)\n",
    "        })\n",
    "        self.df['weight_combined'] = np.zeros(53 * 29)\n",
    "\n",
    "    def update_with_mean(self, df1, df2):\n",
    "        \"\"\"\n",
    "        Update the internal DataFrame with means of turtles_rescued, weight_week,\n",
    "        and weight_capture_site from two DataFrames grouped by week_caught and capture_site.\n",
    "\n",
    "        Parameters:\n",
    "        df1 (pd.DataFrame): The DataFrame containing 'week_caught', 'turtles_rescued', 'weight_week'.\n",
    "        df2 (pd.DataFrame): The DataFrame containing 'capture_site', 'turtles_rescued', 'weight_capture_site'.\n",
    "        \"\"\"\n",
    "        # Calculate mean values for df1 grouped by week_caught\n",
    "        if 'week_caught' in df1.columns:\n",
    "            df1_grouped = df1.groupby('week_caught').agg({\n",
    "                'turtles_rescued': 'mean',\n",
    "                'weight_week': 'mean'\n",
    "            }).reset_index()\n",
    "\n",
    "            # Merge the means into the internal DataFrame\n",
    "            self.df = self.df.merge(df1_grouped, on='week_caught', how='left', suffixes=('', '_df1'))\n",
    "            self.df['turtles_rescued'] = self.df[['turtles_rescued', 'turtles_rescued_df1']].mean(axis=1)\n",
    "            self.df['weight_week'] = self.df[['weight_week', 'weight_week_df1']].mean(axis=1)\n",
    "            self.df.drop(columns=['turtles_rescued_df1', 'weight_week_df1'], inplace=True)\n",
    "\n",
    "        # Calculate mean values for df2 grouped by capture_site\n",
    "        if 'capture_site' in df2.columns:\n",
    "            df2_grouped = df2.groupby('capture_site').agg({\n",
    "                'turtles_rescued': 'mean',\n",
    "                'weight_capture_site': 'mean'\n",
    "            }).reset_index()\n",
    "\n",
    "            # Merge the means into the internal DataFrame\n",
    "            self.df = self.df.merge(df2_grouped, on='capture_site', how='left', suffixes=('', '_df2'))\n",
    "            self.df['turtles_rescued'] = self.df[['turtles_rescued', 'turtles_rescued_df2']].mean(axis=1)\n",
    "            self.df['weight_capture_site'] = self.df[['weight_capture_site', 'weight_capture_site_df2']].mean(axis=1)\n",
    "            self.df.drop(columns=['turtles_rescued_df2', 'weight_capture_site_df2'], inplace=True)\n",
    "\n",
    "        # Calculate the combined weight\n",
    "        self.df['weight_combined'] = self.df['weight_week'] + self.df['weight_capture_site']\n",
    "        \n",
    "        def impute_missing(self):\n",
    "         \"\"\"\n",
    "         Impute missing values in the 'turtles_rescued' column using K-Nearest Neighbors (KNN).\n",
    "         \"\"\"\n",
    "         # Extract features for imputation\n",
    "         features = self.df[['year', 'week_caught', 'capture_site']]\n",
    "        \n",
    "         # Initialize KNN Imputer with k=5 (you can adjust k as needed)\n",
    "         imputer = KNNImputer(n_neighbors=5)\n",
    "        \n",
    "         # Impute missing values\n",
    "         self.df['turtles_rescued'] = imputer.fit_transform(features)\n",
    "\n",
    "        \n",
    "        return self.df"
   ],
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T19:10:25.293798Z",
     "start_time": "2024-05-23T19:10:25.290669Z"
    }
   },
   "cell_type": "code",
   "source": "bla = ",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calculate weights for the validation set"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to calculate weights\n",
    "def calculate_weights(y_true, y_pred):\n",
    "    residuals = np.abs(y_true - y_pred)\n",
    "    weights = 1 / (residuals + 1e-5)  # Adding a small value to avoid division by zero\n",
    "    weights /= weights.sum()  # Normalize weights\n",
    "    return weights"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate weights for the validation set\n",
    "weights = calculate_weights(y_val, (lr_pred + rf_pred + gb_pred) / 3)\n",
    "\n",
    "# Weighted predictions\n",
    "final_pred = (lr_pred * weights + rf_pred * weights + gb_pred * weights) / weights.sum()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Final model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "postprocessor ="
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = imb_pipe(\n",
    "    steps=[\n",
    "        ('fl_imputer', flipper_length_imputer),\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LinearRegression())\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
