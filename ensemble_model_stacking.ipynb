{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test Dataset formating"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "final_test_df = pd.read_csv('data/Sample_sub.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "test_df = pd.read_csv('data/Sample_sub.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "test_df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_capture_site(df):\n",
    "    df['capture_site'] = df['ID'].apply(lambda x: x.split('_')[-2])\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_and_convert_week(df):\n",
    "    # Extract the second to last element\n",
    "    df['week_caught'] = df['ID'].apply(lambda x: x[-2:])\n",
    "\n",
    "    # Convert to datetime with appropriate format for year and month (\"%Y%m\")\n",
    "    df['week_caught'] = df['week_caught'].apply(lambda x : int(x))\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def formating_sample_sub(df): \n",
    "    # extracting capture site id\n",
    "    df = extract_capture_site(df)\n",
    "    \n",
    "    # extracting week of rascue\n",
    "    df = extract_and_convert_week(df)\n",
    "    \n",
    "    # renaming columns to match training set\n",
    "    df.rename(columns={'Capture_Number': 'turtles_rescued'}, inplace=True)\n",
    "    \n",
    "    # getting rid of mixed column\n",
    "    df.drop(columns=['ID'], inplace=True)\n",
    "    \n",
    "    # Standartising prediction \n",
    "    df = df.groupby(['capture_site', 'week_caught'])['turtles_rescued'].sum().reset_index()\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_df = formating_sample_sub(test_df)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_df.to_csv('data/test_df.csv', index=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_df = pd.read_csv('data/test_df.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train Dataset Formating"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "train_df = pd.read_csv('data/train.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "train_df.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "# Defining function to Standartising column names \n",
    "def standardize_column_names(col):\n",
    "    # Replace spaces with underscores\n",
    "    col = col.replace(' ', '_')\n",
    "    # Insert underscore before each uppercase letter preceded by a lowercase letter or followed by a lowercase letter\n",
    "    col = re.sub(r'(?<=[a-z])(?=[A-Z])', '_', col)\n",
    "    col = re.sub(r'(?<=[A-Z])(?=[A-Z][a-z])', '_', col)\n",
    "    # Convert to lower case\n",
    "    col = col.lower()\n",
    "    # Ensure single underscores only (in case of consecutive underscores from initial spaces)\n",
    "    col = re.sub(r'_+', '_', col)\n",
    "    return col\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Applying function to df\n",
    "train_df.columns = [standardize_column_names(col) for col in train_df.columns]\n",
    "\n",
    "# Printing the updated column names to verify the changes\n",
    "print(train_df.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the function to extract the number\n",
    "def extract_number_split(s):\n",
    "    return int(s.split('_')[-1])\n",
    "\n",
    "# Define a function to apply the extraction to multiple columns\n",
    "def apply_extraction(df, columns):\n",
    "    for column in columns:\n",
    "        # Convert column to string type if it's not already\n",
    "        if df[column].dtype != 'object':\n",
    "            df[column] = df[column].astype(str)\n",
    "\n",
    "        # Apply the extraction function\n",
    "        df[column] = df[column].apply(extract_number_split)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "columns_to_extract_train = ['fisher', 'researcher', 'capture_site', 'species']\n",
    "train_df = apply_extraction(train_df, columns_to_extract_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns_to_drop = ['rescue_id', 'turtle_characteristics', 'tag_1', 'tag_2', 'lost_tags', 't_number', 'sex',\n",
    "                   'capture_method', 'release_site', 'landing_site', 'status', 'foraging_ground', 'date_time_release']\n",
    "\n",
    "train_df = train_df.drop(columns=columns_to_drop)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def convert_and_split_datetime(df, columns):\n",
    "    \"\"\"\n",
    "    Convert specified datetime columns to timestamp and split into year and week columns\n",
    "    with new names based on the original column names.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the columns.\n",
    "    columns (list): List of column names to convert and split.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with new year and week columns.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        # Convert the column to datetime\n",
    "        df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "\n",
    "        # Extract the base name without 'date_time_' prefix\n",
    "        base_name = column.replace('date_time_', '')\n",
    "\n",
    "        # Create new columns for year and week with the desired names\n",
    "        df[f'year_{base_name}'] = df[column].dt.year\n",
    "        df[f'week_{base_name}'] = df[column].dt.isocalendar().week\n",
    "\n",
    "        # Drop the original datetime column if desired\n",
    "        df.drop(columns=[column], inplace=True)\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Apply function to train_df\n",
    "columns_to_convert = ['date_time_caught']\n",
    "train_df = convert_and_split_datetime(train_df, columns_to_convert)\n",
    "\n",
    "train_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imputing Missing Data in Weight"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "def imput_missing_weight_values(df, n = 5):\n",
    "    knn_df = df[['ccl_cm', 'ccw_cm', 'weight_kg']]\n",
    "    imputer = KNNImputer(n_neighbors=n)\n",
    "    imputer.set_output(transform='pandas')\n",
    "\n",
    "    return imputer.fit_transform(knn_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "imputed_df = imput_missing_weight_values(train_df)\n",
    "train_df['ccl_cm'] = imputed_df['ccl_cm']\n",
    "train_df['ccw_cm'] = imputed_df['ccw_cm']\n",
    "train_df['weight_kg'] = imputed_df['weight_kg']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df.info()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df.to_csv('data/train.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df = pd.read_csv('data/train.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Baseline Model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "baseline_df = train_df.groupby(['year_caught', 'capture_site', 'week_caught']).size().reset_index(name='turtles_rescued')\n",
    "baseline_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "baseline_df = baseline_df[~baseline_df['year_caught'].between(1988, 2006)].reset_index(drop=True)\n",
    "baseline_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "baseline_df.drop(['year_caught'], axis=1, inplace=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "baseline_test = test_df.copy()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BaselinePredictor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def predict_turtles_rescued_all(self):\n",
    "        # Initialize an empty list to store dataframes\n",
    "        dfs = []\n",
    "\n",
    "        # Get unique capture sites and weeks in the baseline_df\n",
    "        capture_site_all = self.df['capture_site'].unique()\n",
    "        weeks_in_year = self.df['week_caught'].unique()\n",
    "\n",
    "        # Iterate over each capture site and week to calculate the mean turtles_rescued\n",
    "        for capture_site in capture_site_all:\n",
    "            for week in weeks_in_year:\n",
    "                # Calculate mean turtles_rescued for the current capture site and week\n",
    "                mean_turtles_rescued = self.df[(self.df['capture_site'] == capture_site) & (self.df['week_caught'] == week)]['turtles_rescued'].mean()\n",
    "\n",
    "                # Append a dataframe to the list\n",
    "                dfs.append(pd.DataFrame({'capture_site': [capture_site], 'week_caught': [week], 'turtles_rescued': [mean_turtles_rescued]}))\n",
    "\n",
    "        # Concatenate all dataframes in the list\n",
    "        predict_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        return predict_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the predictor with the baseline_df\n",
    "predictor = BaselinePredictor(baseline_df)\n",
    "\n",
    "# Predict the baseline values\n",
    "predict_baseline = predictor.predict_turtles_rescued_all()\n",
    "\n",
    "# Print the predicted baseline DataFrame\n",
    "print(predict_baseline)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Ensure both DataFrames have the same number of rows\n",
    "num_rows_baseline_test = baseline_test.shape[0]\n",
    "\n",
    "# Randomly sample rows from predict_baseline to match the number of rows in sample_sub\n",
    "predict_baseline_trimmed = predict_baseline.sample(n=num_rows_baseline_test, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Ensure the indices match\n",
    "baseline_test = baseline_test.reset_index(drop=True)\n",
    "\n",
    "# Combine both DataFrames to ensure we drop NaNs in corresponding rows\n",
    "combined_df = pd.concat([baseline_test['turtles_rescued'], predict_baseline_trimmed['turtles_rescued']], axis=1, keys=['true', 'pred'])\n",
    "\n",
    "# Drop rows with NaN values in either column\n",
    "combined_df = combined_df.dropna()\n",
    "\n",
    "# Separate the true and predicted values\n",
    "y_true = combined_df['true']\n",
    "y_pred = combined_df['pred']\n",
    "\n",
    "# Calculate MAE\n",
    "mae_baseline = mean_absolute_error(y_true, y_pred)\n",
    "print(mae_baseline)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Contextualising MAE"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(baseline_test.turtles_rescued.min())\n",
    "print(baseline_test.turtles_rescued.max())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_min = 1\n",
    "target_max = 9\n",
    "target_range = target_max - target_min\n",
    "acceptable_mae = target_range * 0.1  # Example threshold of 10% of the range\n",
    "print(f\"Acceptable MAE: {acceptable_mae}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RMSE Baseline"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(test_df['turtles_rescued'].describe())\n",
    "\n",
    "# Baseline RMSE\n",
    "mean_turtles_rescued = baseline_test['turtles_rescued'].mean()\n",
    "baseline_predictions = [mean_turtles_rescued] * len(test_df)\n",
    "baseline_mse = mean_squared_error(baseline_test['turtles_rescued'], baseline_predictions)\n",
    "baseline_rmse = np.sqrt(baseline_mse)\n",
    "print(f\"Baseline RMSE: {baseline_rmse}\")\n",
    "\n",
    "# Coefficient of Variation of RMSE\n",
    "cv_rmse = (rmse / mean_turtles_rescued) * 100\n",
    "print(f\"Coefficient of Variation of RMSE: {cv_rmse:.2f}%\")\n",
    "\n",
    "# Standard Deviation of Turtles Rescued\n",
    "std_turtles_rescued = baseline_test['turtles_rescued'].std()\n",
    "print(f\"Standard Deviation of Turtles Rescued: {std_turtles_rescued}\")\n",
    "print(f\"RMSE as a proportion of Standard Deviation: {rmse / std_turtles_rescued}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Benchmarking against baseline"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the MAE for the baseline model\n",
    "baseline_mae = mean_absolute_error(test_df['turtles_rescued'], baseline_predictions)\n",
    "print(f\"Baseline MAE: {baseline_mae}\")\n",
    "\n",
    "# Compare baseline MAE with your model's MAE\n",
    "if mae_baseline < baseline_mae:\n",
    "    print(\"Your model is performing better than the baseline.\")\n",
    "else:\n",
    "    print(\"Your model is not performing better than the baseline.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Interactive version"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_turtles_rescued(capture_site, week):\n",
    "    predict_df = baseline_df[(baseline_df['capture_site'] == 5) & (baseline_df['week_caught'] == 5)]\n",
    "    turtle_rescued = predict_df['turtles_rescued'].mean()\n",
    "    return turtle_rescued"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "week = input(f'Enter a week number for which you would like to predict (from 1 to {len(baseline_df.week_caught.unique())}):')\n",
    "capture_site = input(f'Enter a capture site number (from 1 to 29):')\n",
    "print(f'Predicted Turtles Rescued for {capture_site} and {week}: {predict_turtles_rescued(capture_site, week)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Enseble Model Staking"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformin data for round 1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "to_transform_train = train_df.groupby(['year_caught', 'capture_site', 'week_caught']).size().reset_index(name='turtles_rescued')\n",
    "to_transform_test = test_df.copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TurtleRescueModifierTrain:\n",
    "    def __init__(self):\n",
    "        # Generate arrays for year, week_caught, and capture_site\n",
    "        years = np.repeat(np.arange(1988, 2019), 53 * 29)\n",
    "        week_caught = np.tile(np.arange(1, 54), 31 * 29)\n",
    "        capture_site = np.repeat(np.arange(1, 30), 53 * 31)\n",
    "\n",
    "        # Create the DataFrame\n",
    "        self.df = pd.DataFrame({\n",
    "            'year_caught': years,\n",
    "            'week_caught': week_caught,\n",
    "            'capture_site': capture_site,\n",
    "            'turtles_rescued': np.zeros(len(years))\n",
    "        })\n",
    "\n",
    "    def merge_data(self, source_df):\n",
    "        for index, row in source_df.iterrows():\n",
    "            # Match conditions based on year_caught, week_caught, and capture_site\n",
    "            match_condition = (\n",
    "                    (self.df['year_caught'] == row['year_caught']) &\n",
    "                    (self.df['week_caught'] == row['week_caught']) &\n",
    "                    (self.df['capture_site'] == row['capture_site'])\n",
    "            )\n",
    "\n",
    "            # Check if a matching row exists in the target_df\n",
    "            matching_row_index = self.df.index[match_condition]\n",
    "\n",
    "            if len(matching_row_index) > 0:\n",
    "                # Update the existing row in target_df with data from source_df\n",
    "                self.df.loc[matching_row_index[0], 'turtles_rescued'] = row['turtles_rescued']\n",
    "            else:\n",
    "                # If no matching row exists, create a new row in target_df\n",
    "                new_row = {\n",
    "                    'year_caught': row['year_caught'],\n",
    "                    'week_caught': row['week_caught'],\n",
    "                    'capture_site': row['capture_site'],\n",
    "                    'turtles_rescued': row['turtles_rescued']\n",
    "                }\n",
    "                self.df = pd.concat([self.df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def impute_missing_values(self):\n",
    "        \"\"\"\n",
    "        Impute missing values in the 'turtles_rescued' column using K-Nearest Neighbors (KNN).\n",
    "        \"\"\"\n",
    "        # Ensure some 'turtles_rescued' values are NaN for imputation demonstration\n",
    "        self.df.loc[self.df.sample(frac=0.1).index, 'turtles_rescued'] = np.nan\n",
    "\n",
    "        # Select columns for imputation\n",
    "        features = self.df[['year_caught', 'week_caught', 'capture_site']]\n",
    "        targets = self.df[['turtles_rescued']]\n",
    "\n",
    "        # Combine features and targets for imputation\n",
    "        combined = pd.concat([features, targets], axis=1)\n",
    "\n",
    "        # Initialize KNN Imputer with k=5\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "        # Impute missing values\n",
    "        imputed_data = imputer.fit_transform(combined)\n",
    "        self.df['turtles_rescued'] = imputed_data[:, -1]\n",
    "        \n",
    "        # Convert 'turtles_rescued' to integer type\n",
    "        self.df['turtles_rescued'] = self.df['turtles_rescued'].astype(int)\n",
    "        return self.df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "merged_train = TurtleRescueModifierTrain()\n",
    "merged_train.merge_data(to_transform_train)\n",
    "imputed_train = TurtleRescueModifierTrain.impute_missing_values(merged_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TurtleRescueModifierTest:\n",
    "    def __init__(self, year):\n",
    "        \"\"\"\n",
    "        Initialize the TurtleRescueModifierTrain with data for a specific year.\n",
    "        \n",
    "        :param year: The specific year for which to initialize the data.\n",
    "        \"\"\"\n",
    "        # Generate arrays for the specified year, week_caught, and capture_site\n",
    "        self.year = year\n",
    "        weeks_per_year = 53\n",
    "        capture_sites = 29\n",
    "\n",
    "        year = np.repeat(year, weeks_per_year * capture_sites)\n",
    "        week_caught = np.tile(np.arange(1, weeks_per_year + 1), capture_sites)\n",
    "        capture_site = np.repeat(np.arange(1, capture_sites + 1), weeks_per_year)\n",
    "\n",
    "        # Create the DataFrame\n",
    "        self.df = pd.DataFrame({\n",
    "            'year_caught': year,\n",
    "            'week_caught': week_caught,\n",
    "            'capture_site': capture_site,\n",
    "            'turtles_rescued': np.zeros(len(year))\n",
    "        })\n",
    "\n",
    "    def merge_data(self, source_df):\n",
    "        # Add 'year_caught' column to source_df if it doesn't exist\n",
    "        if 'year_caught' not in source_df.columns:\n",
    "            source_df['year_caught'] = self.year\n",
    "        \n",
    "        for index, row in source_df.iterrows():\n",
    "            # Match conditions based on year_caught, week_caught, and capture_site\n",
    "            match_condition = (\n",
    "                (self.df['year_caught'] == row['year_caught']) &\n",
    "                (self.df['week_caught'] == row['week_caught']) &\n",
    "                (self.df['capture_site'] == row['capture_site'])\n",
    "            )\n",
    "\n",
    "            # Check if a matching row exists in the target_df\n",
    "            matching_row_index = self.df.index[match_condition]\n",
    "\n",
    "            if len(matching_row_index) > 0:\n",
    "                # Update the existing row in target_df with data from source_df\n",
    "                self.df.loc[matching_row_index[0], 'turtles_rescued'] = row['turtles_rescued']\n",
    "            else:\n",
    "                # If no matching row exists, create a new row in target_df\n",
    "                new_row = {\n",
    "                    'year_caught': row['year_caught'],\n",
    "                    'week_caught': row['week_caught'],\n",
    "                    'capture_site': row['capture_site'],\n",
    "                    'turtles_rescued': row['turtles_rescued']\n",
    "                }\n",
    "                self.df = pd.concat([self.df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "    def impute_missing_values(self):\n",
    "        \"\"\"\n",
    "        Impute missing values in the 'turtles_rescued' column using K-Nearest Neighbors (KNN).\n",
    "        \"\"\"\n",
    "        # Introduce NaN values into the 'turtles_rescued' column to demonstrate imputation\n",
    "        self.df.loc[self.df.sample(frac=0.1).index, 'turtles_rescued'] = np.nan\n",
    "\n",
    "        # Initialize KNN Imputer with k=5\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "        # Select columns for imputation\n",
    "        features = self.df[['year_caught', 'week_caught', 'capture_site']]\n",
    "        targets = self.df[['turtles_rescued']]\n",
    "\n",
    "        # Combine features and targets for imputation\n",
    "        combined = pd.concat([features, targets], axis=1)\n",
    "\n",
    "        # Impute missing values\n",
    "        imputed_data = imputer.fit_transform(combined)\n",
    "        self.df['turtles_rescued'] = imputed_data[:, -1]\n",
    "\n",
    "        # Convert 'turtles_rescued' to integer type\n",
    "        self.df['turtles_rescued'] = self.df['turtles_rescued'].astype(int)\n",
    "\n",
    "        return self.df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "merged_test = TurtleRescueModifierTest(year = 2019)\n",
    "merged_test.merge_data(to_transform_test)\n",
    "imputed_test = merged_test.impute_missing_values()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TurtleRescueContainer:\n",
    "    def __init__(self):\n",
    "        # Initialize the DataFrame with 53 weeks and 29 capture sites\n",
    "        self.df = pd.DataFrame({\n",
    "            'week_caught': np.tile(np.arange(1, 54), 29),\n",
    "            'capture_site': np.repeat(np.arange(1, 30), 53),\n",
    "            'turtles_rescued': np.zeros(53 * 29),\n",
    "            'weight_week': np.zeros(53 * 29),\n",
    "            'weight_capture_site': np.zeros(53 * 29)\n",
    "        })\n",
    "        self.df['weight_combined'] = np.zeros(53 * 29)\n",
    "\n",
    "    def update_with_mean(self, df1, df2):\n",
    "        \"\"\"\n",
    "        Update the internal DataFrame with means of turtles_rescued, weight_week,\n",
    "        and weight_capture_site from two DataFrames grouped by week_caught and capture_site.\n",
    "\n",
    "        Parameters:\n",
    "        df1 (pd.DataFrame): The DataFrame containing 'week_caught', 'turtles_rescued', 'weight_week'.\n",
    "        df2 (pd.DataFrame): The DataFrame containing 'capture_site', 'turtles_rescued', 'weight_capture_site'.\n",
    "        \"\"\"\n",
    "        # Calculate mean values for df1 grouped by week_caught\n",
    "        if 'week_caught' in df1.columns:\n",
    "            df1_grouped = df1.groupby('week_caught').agg({\n",
    "                'turtles_rescued': 'mean',\n",
    "                'weight_week': 'mean'\n",
    "            }).reset_index()\n",
    "\n",
    "            # Merge the means into the internal DataFrame\n",
    "            self.df = self.df.merge(df1_grouped, on='week_caught', how='left', suffixes=('', '_df1'))\n",
    "            self.df['turtles_rescued'] = self.df[['turtles_rescued', 'turtles_rescued_df1']].mean(axis=1)\n",
    "            self.df['weight_week'] = self.df[['weight_week', 'weight_week_df1']].mean(axis=1)\n",
    "            self.df.drop(columns=['turtles_rescued_df1', 'weight_week_df1'], inplace=True)\n",
    "\n",
    "        # Calculate mean values for df2 grouped by capture_site\n",
    "        if 'capture_site' in df2.columns:\n",
    "            df2_grouped = df2.groupby('capture_site').agg({\n",
    "                'turtles_rescued': 'mean',\n",
    "                'weight_capture_site': 'mean'\n",
    "            }).reset_index()\n",
    "\n",
    "            # Merge the means into the internal DataFrame\n",
    "            self.df = self.df.merge(df2_grouped, on='capture_site', how='left', suffixes=('', '_df2'))\n",
    "            self.df['turtles_rescued'] = self.df[['turtles_rescued', 'turtles_rescued_df2']].mean(axis=1)\n",
    "            self.df['weight_capture_site'] = self.df[['weight_capture_site', 'weight_capture_site_df2']].mean(axis=1)\n",
    "            self.df.drop(columns=['turtles_rescued_df2', 'weight_capture_site_df2'], inplace=True)\n",
    "\n",
    "        # Calculate the combined weight\n",
    "        self.df['weight_combined'] = self.df['weight_week'] + self.df['weight_capture_site']\n",
    "        \n",
    "        def impute_missing(self):\n",
    "         \"\"\"\n",
    "         Impute missing values in the 'turtles_rescued' column using K-Nearest Neighbors (KNN).\n",
    "         \"\"\"\n",
    "         # Extract features for imputation\n",
    "         features = self.df[['year', 'week_caught', 'capture_site']]\n",
    "        \n",
    "         # Initialize KNN Imputer with k=5 (you can adjust k as needed)\n",
    "         imputer = KNNImputer(n_neighbors=5)\n",
    "        \n",
    "         # Impute missing values\n",
    "         self.df['turtles_rescued'] = imputer.fit_transform(features)\n",
    "\n",
    "        \n",
    "        return self.df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simple ML model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:52:04.034531Z",
     "start_time": "2024-05-23T22:49:24.435202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# loading data for model\n",
    "train_simple = imputed_train.copy()\n",
    "test_simple = imputed_test.copy()\n",
    "\n",
    "# Extract features and target from train_week DataFrame\n",
    "X_train_simple = train_simple[['year_caught', 'week_caught', 'capture_site']]\n",
    "y_train_simple = train_simple['turtles_rescued']\n",
    "\n",
    "X_test_simple = test_simple[['year_caught', 'week_caught', 'capture_site']]\n",
    "y_test_simple = ['turtles_rescued']\n",
    "\n",
    "# Define a list of regression models\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(),\n",
    "    Lasso(),\n",
    "    DecisionTreeRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    SVR()\n",
    "]\n",
    "\n",
    "# Define a list to store the trained models\n",
    "trained_models_simple = []\n",
    "\n",
    "# Iterate over each model and fit it to the training data\n",
    "for model in models:\n",
    "    model.fit(X_train_simple, y_train_simple)\n",
    "    trained_models_simple.append(model)\n",
    "\n",
    "# Perform cross-validation for each model and evaluate their performance\n",
    "for model in models:\n",
    "    scores_week_caught = cross_val_score(model, X_train_simple, y_train_simple, scoring='neg_mean_squared_error', cv=5)\n",
    "    rmse_scores_week_caught = np.sqrt(-scores_week_caught)\n",
    "    print(f\"{model.__class__.__name__}: Mean RMSE: {rmse_scores_week_caught.mean()}, Std RMSE: {rmse_scores_week_caught.std()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression: Mean RMSE: 0.7755586975741101, Std RMSE: 0.9123569095216355\n",
      "Ridge: Mean RMSE: 0.77555857499529, Std RMSE: 0.9123565437055333\n",
      "Lasso: Mean RMSE: 0.8568197695076567, Std RMSE: 0.8799947970367469\n",
      "DecisionTreeRegressor: Mean RMSE: 1.7478028821827514, Std RMSE: 0.7129092440924288\n",
      "RandomForestRegressor: Mean RMSE: 1.5572672141203443, Std RMSE: 0.6232480063598077\n",
      "SVR: Mean RMSE: 0.6744766983019547, Std RMSE: 0.9261452350434871\n"
     ]
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predicting turtles_rescued by week"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# loading data for model\n",
    "train_week_caught = imputed_train.copy()\n",
    "test_week_caught = imputed_test.copy()\n",
    "\n",
    "# Extract features and target from train_week DataFrame\n",
    "X_train_week_caught = train_week_caught[['year_caught', 'week_caught', 'capture_site']]\n",
    "y_train_week_caught = train_week_caught['turtles_rescued']\n",
    "\n",
    "X_test_week_caught = test_week_caught[['year_caught', 'week_caught', 'capture_site']]\n",
    "y_test_week_caught = test_week_caught['turtles_rescued']\n",
    "\n",
    "\n",
    "# Define a list to store the trained models\n",
    "trained_models_week_caught = []\n",
    "\n",
    "# Iterate over each model and fit it to the training data\n",
    "for model in models:\n",
    "    model.fit(X_train_week_caught, y_train_week_caught)\n",
    "    trained_models_week_caught.append(model)\n",
    "\n",
    "# Perform cross-validation for each model and evaluate their performance\n",
    "for model in models:\n",
    "    scores_week_caught = cross_val_score(model, X_train_week_caught, y_train_week_caught, scoring='neg_mean_squared_error', cv=5)\n",
    "    rmse_scores_week_caught = np.sqrt(-scores_week_caught)\n",
    "    print(f\"{model.__class__.__name__}: Mean RMSE: {rmse_scores_week_caught.mean()}, Std RMSE: {rmse_scores_week_caught.std()}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predicting what turtles are caught by capture_site"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# loading data for model\n",
    "train_capture_site = imputed_train.copy()\n",
    "test_capture_site = imputed_test.copy()\n",
    "\n",
    "X_train_capture_site = train_capture_site[['year_caught', 'week_caught', 'capture_site']]\n",
    "y_train_capture_site = train_capture_site['turtles_rescued']\n",
    "\n",
    "X_test_capture_site = test_capture_site[['year_caught', 'week_caught', 'capture_site']]\n",
    "y_test_capture_site = test_capture_site['turtles_rescued']\n",
    "\n",
    "# Define a list to store the trained models\n",
    "trained_models_capture_site = []\n",
    "\n",
    "# Iterate over each model and fit it to the training data\n",
    "for model in models:\n",
    "    model.fit(X_train_capture_site, y_train_capture_site)\n",
    "    trained_models_capture_site.append(model)\n",
    "\n",
    "# Perform cross-validation for each model and evaluate their performance\n",
    "for model in models:\n",
    "    scores_capture_site = cross_val_score(model, X_train_capture_site, y_train_capture_site,\n",
    "                                          scoring='neg_mean_squared_error', cv=5)\n",
    "    rmse_scores_capture_site = np.sqrt(-scores_capture_site)\n",
    "    print(\n",
    "        f\"{model.__class__.__name__}: Mean RMSE: {rmse_scores_capture_site.mean()}, Std RMSE: {rmse_scores_capture_site.std()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calculate weights for the validation set"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Function to calculate weights\n",
    "def calculate_weights(y_true, y_pred):\n",
    "    residuals = np.abs(y_true - y_pred)\n",
    "    weights = 1 / (residuals + 1e-5)  # Adding a small value to avoid division by zero\n",
    "    weights /= weights.sum()  # Normalize weights\n",
    "    return weights"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Calculate weights for the validation set\n",
    "weights = calculate_weights(y_val, (lr_pred + rf_pred + gb_pred) / 3)\n",
    "\n",
    "# Weighted predictions\n",
    "final_pred = (lr_pred * weights + rf_pred * weights + gb_pred * weights) / weights.sum()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Final model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:35:58.832639Z",
     "start_time": "2024-05-23T22:34:28.206900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train = imputed_train[['year_caught', 'week_caught', 'capture_site']]\n",
    "y_train = imputed_train['turtles_rescued']\n",
    "\n",
    "X_test = imputed_test[['year_caught', 'week_caught', 'capture_site']]\n",
    "y_test = imputed_test.turtles_rescued.copy()\n",
    "\n",
    "# Generate predictions using trained models for capture_site\n",
    "predictions_capture_site_train = np.column_stack([model.predict(X_train_capture_site) for model in trained_models_capture_site])\n",
    "\n",
    "# Generate predictions using trained models for week_caught\n",
    "predictions_week_caught_train = np.column_stack([model.predict(X_train_week_caught) for model in trained_models_week_caught])\n",
    "\n",
    "# Generate predictions using trained models for capture_site\n",
    "predictions_capture_site_test = np.column_stack([model.predict(X_test_capture_site) for model in trained_models_capture_site])\n",
    "\n",
    "# Generate predictions using trained models for week_caught\n",
    "predictions_week_caught_test = np.column_stack([model.predict(X_test_week_caught) for model in trained_models_week_caught])"
   ],
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:38:54.902628Z",
     "start_time": "2024-05-23T22:38:54.895001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Combine the predictions into a single feature set\n",
    "combined_predictions_train = np.concatenate((predictions_capture_site_train, predictions_week_caught_train), axis=1)\n",
    "combined_predictions_test = np.concatenate((predictions_capture_site_test, predictions_week_caught_test), axis=1)"
   ],
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:53:29.871451Z",
     "start_time": "2024-05-23T22:52:04.036774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train a new model (e.g., Random Forest) on the combined predictions\n",
    "combined_model = SVR()\n",
    "combined_model.fit(combined_predictions_train, y_train)  # Assuming y_test is the target variable\n",
    "\n",
    "# Use the combined model to make predictions\n",
    "combined_predictions_train_final = combined_model.predict(combined_predictions_train)\n",
    "combined_predictions_test_final = combined_model.predict(combined_predictions_test)\n",
    "\n",
    "# Calculate RMSE for predictions made by the combined model on the test data\n",
    "rmse_combined_test = np.sqrt(mean_squared_error(y_test, combined_predictions_test_final))\n",
    "print(f\"Combined Model RMSE on Test Data: {rmse_combined_test}\")\n",
    "\n",
    "# Perform cross-validation for the combined model on the training data\n",
    "cv_scores_combined = cross_val_score(combined_model, combined_predictions_train, y_train,\n",
    "                                     scoring='neg_mean_squared_error', cv=5)\n",
    "rmse_cv_combined = np.sqrt(-cv_scores_combined)\n",
    "print(f\"Combined Model Cross-Validation RMSE: Mean={rmse_cv_combined.mean()}, Std={rmse_cv_combined.std()}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Model RMSE on Test Data: 3.884347895389471\n",
      "Combined Model Cross-Validation RMSE: Mean=0.666241684750007, Std=0.9121731389277432\n"
     ]
    }
   ],
   "execution_count": 172
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
