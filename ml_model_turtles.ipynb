{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.097081Z",
     "start_time": "2024-05-23T22:57:36.094767Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test Dataset formating"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.350282Z",
     "start_time": "2024-05-23T22:57:36.344836Z"
    }
   },
   "cell_type": "code",
   "source": "final_test_df = pd.read_csv('data/Sample_sub.csv')",
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.354279Z",
     "start_time": "2024-05-23T22:57:36.351359Z"
    }
   },
   "source": "test_df = pd.read_csv('data/Sample_sub.csv')",
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.372419Z",
     "start_time": "2024-05-23T22:57:36.367681Z"
    }
   },
   "source": "test_df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                        ID  Capture_Number\n",
       "0     CaptureSite_0_201901               7\n",
       "1     CaptureSite_0_201902               1\n",
       "2     CaptureSite_0_201903               5\n",
       "3     CaptureSite_0_201904               2\n",
       "4     CaptureSite_0_201905               3\n",
       "...                    ...             ...\n",
       "1271  CaptureSite_9_201940               0\n",
       "1272  CaptureSite_9_201941               7\n",
       "1273  CaptureSite_9_201942               7\n",
       "1274  CaptureSite_9_201943               3\n",
       "1275  CaptureSite_9_201944               5\n",
       "\n",
       "[1276 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Capture_Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CaptureSite_0_201901</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CaptureSite_0_201902</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CaptureSite_0_201903</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CaptureSite_0_201904</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CaptureSite_0_201905</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>CaptureSite_9_201940</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>CaptureSite_9_201941</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>CaptureSite_9_201942</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>CaptureSite_9_201943</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>CaptureSite_9_201944</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1276 rows Ã— 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.413999Z",
     "start_time": "2024-05-23T22:57:36.412095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_capture_site(df):\n",
    "    df['capture_site'] = df['ID'].apply(lambda x: x.split('_')[-2])\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.425612Z",
     "start_time": "2024-05-23T22:57:36.423687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_and_convert_week(df):\n",
    "    # Extract the second to last element\n",
    "    df['week_caught'] = df['ID'].apply(lambda x: x[-2:])\n",
    "\n",
    "    # Convert to datetime with appropriate format for year and month (\"%Y%m\")\n",
    "    df['week_caught'] = df['week_caught'].apply(lambda x : int(x))\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.467735Z",
     "start_time": "2024-05-23T22:57:36.465173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def formating_sample_sub(df): \n",
    "    # extracting capture site id\n",
    "    df = extract_capture_site(df)\n",
    "    \n",
    "    # extracting week of rascue\n",
    "    df = extract_and_convert_week(df)\n",
    "    \n",
    "    # renaming columns to match training set\n",
    "    df.rename(columns={'Capture_Number': 'turtles_rescued'}, inplace=True)\n",
    "    \n",
    "    # getting rid of mixed column\n",
    "    df.drop(columns=['ID'], inplace=True)\n",
    "    \n",
    "    # Standartising prediction \n",
    "    df = df.groupby(['capture_site', 'week_caught'])['turtles_rescued'].sum().reset_index()\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.504370Z",
     "start_time": "2024-05-23T22:57:36.499251Z"
    }
   },
   "cell_type": "code",
   "source": "test_df = formating_sample_sub(test_df)",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.625183Z",
     "start_time": "2024-05-23T22:57:36.622298Z"
    }
   },
   "cell_type": "code",
   "source": "test_df = pd.read_csv('data/test_df.csv')",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train Dataset Formating"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.720279Z",
     "start_time": "2024-05-23T22:57:36.661212Z"
    }
   },
   "source": "train_df = pd.read_csv('data/train.csv')",
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.727829Z",
     "start_time": "2024-05-23T22:57:36.721299Z"
    }
   },
   "source": "train_df.head()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Rescue_ID Date_TimeCaught     Researcher    CaptureSite ForagingGround   \n",
       "0  2000_RE_0060      2000-12-22  Researcher_25  CaptureSite_0          Ocean  \\\n",
       "1  2001_RE_0187      2001-10-28   Researcher_6  CaptureSite_0          Ocean   \n",
       "2  2001_RE_0197      2001-11-01   Researcher_6  CaptureSite_0          Ocean   \n",
       "3  2002_RE_0031      2002-03-11  Researcher_32  CaptureSite_0          Ocean   \n",
       "4  2002_RE_0118      2002-08-08  Researcher_25  CaptureSite_0          Ocean   \n",
       "\n",
       "  CaptureMethod       Fisher                        LandingSite    Species   \n",
       "0           Net  Fisher_1072  LandingSite_CaptureSiteCategory_2  Species_6  \\\n",
       "1           Net   Fisher_520  LandingSite_CaptureSiteCategory_2  Species_6   \n",
       "2           Net  Fisher_1669  LandingSite_CaptureSiteCategory_2  Species_5   \n",
       "3           Net  Fisher_1798  LandingSite_CaptureSiteCategory_2  Species_6   \n",
       "4       Beached  Fisher_1918  LandingSite_CaptureSiteCategory_2  Species_5   \n",
       "\n",
       "            Tag_1  ... Lost_Tags T_Number CCL_cm  CCW_cm  Weight_Kg      Sex   \n",
       "0         CC00147  ...       NaN      NaN  64.70   62.60        NaN  Unknown  \\\n",
       "1            W442  ...       NaN      NaN  35.85   31.35        NaN  Unknown   \n",
       "2          KE0376  ...       NaN      NaN  51.80   49.20        NaN  Unknown   \n",
       "3         CC00302  ...       NaN      NaN  60.50   59.00        NaN  Unknown   \n",
       "4  NotTagged_0113  ...       NaN      NaN  34.70   33.00        NaN  Unknown   \n",
       "\n",
       "                               TurtleCharacteristics    Status   \n",
       "0                             algae at rear of shell  Released  \\\n",
       "1  multiple b's on front flippers&  a lot of alga...  Released   \n",
       "2                                              clean  Released   \n",
       "3  1 b 3 CS+ calcerous algae at rear end of shell...  Released   \n",
       "4  very lively+ right eye is hanging out + swolle...  Released   \n",
       "\n",
       "      ReleaseSite Date_TimeRelease  \n",
       "0  ReleaseSite_50         22/12/00  \n",
       "1  ReleaseSite_62         28/10/01  \n",
       "2  ReleaseSite_50         01/11/01  \n",
       "3  ReleaseSite_50         11/03/02  \n",
       "4  ReleaseSite_62         08/08/02  \n",
       "\n",
       "[5 rows x 21 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rescue_ID</th>\n",
       "      <th>Date_TimeCaught</th>\n",
       "      <th>Researcher</th>\n",
       "      <th>CaptureSite</th>\n",
       "      <th>ForagingGround</th>\n",
       "      <th>CaptureMethod</th>\n",
       "      <th>Fisher</th>\n",
       "      <th>LandingSite</th>\n",
       "      <th>Species</th>\n",
       "      <th>Tag_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Lost_Tags</th>\n",
       "      <th>T_Number</th>\n",
       "      <th>CCL_cm</th>\n",
       "      <th>CCW_cm</th>\n",
       "      <th>Weight_Kg</th>\n",
       "      <th>Sex</th>\n",
       "      <th>TurtleCharacteristics</th>\n",
       "      <th>Status</th>\n",
       "      <th>ReleaseSite</th>\n",
       "      <th>Date_TimeRelease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000_RE_0060</td>\n",
       "      <td>2000-12-22</td>\n",
       "      <td>Researcher_25</td>\n",
       "      <td>CaptureSite_0</td>\n",
       "      <td>Ocean</td>\n",
       "      <td>Net</td>\n",
       "      <td>Fisher_1072</td>\n",
       "      <td>LandingSite_CaptureSiteCategory_2</td>\n",
       "      <td>Species_6</td>\n",
       "      <td>CC00147</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.70</td>\n",
       "      <td>62.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>algae at rear of shell</td>\n",
       "      <td>Released</td>\n",
       "      <td>ReleaseSite_50</td>\n",
       "      <td>22/12/00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001_RE_0187</td>\n",
       "      <td>2001-10-28</td>\n",
       "      <td>Researcher_6</td>\n",
       "      <td>CaptureSite_0</td>\n",
       "      <td>Ocean</td>\n",
       "      <td>Net</td>\n",
       "      <td>Fisher_520</td>\n",
       "      <td>LandingSite_CaptureSiteCategory_2</td>\n",
       "      <td>Species_6</td>\n",
       "      <td>W442</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.85</td>\n",
       "      <td>31.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>multiple b's on front flippers&amp;  a lot of alga...</td>\n",
       "      <td>Released</td>\n",
       "      <td>ReleaseSite_62</td>\n",
       "      <td>28/10/01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001_RE_0197</td>\n",
       "      <td>2001-11-01</td>\n",
       "      <td>Researcher_6</td>\n",
       "      <td>CaptureSite_0</td>\n",
       "      <td>Ocean</td>\n",
       "      <td>Net</td>\n",
       "      <td>Fisher_1669</td>\n",
       "      <td>LandingSite_CaptureSiteCategory_2</td>\n",
       "      <td>Species_5</td>\n",
       "      <td>KE0376</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.80</td>\n",
       "      <td>49.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>clean</td>\n",
       "      <td>Released</td>\n",
       "      <td>ReleaseSite_50</td>\n",
       "      <td>01/11/01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002_RE_0031</td>\n",
       "      <td>2002-03-11</td>\n",
       "      <td>Researcher_32</td>\n",
       "      <td>CaptureSite_0</td>\n",
       "      <td>Ocean</td>\n",
       "      <td>Net</td>\n",
       "      <td>Fisher_1798</td>\n",
       "      <td>LandingSite_CaptureSiteCategory_2</td>\n",
       "      <td>Species_6</td>\n",
       "      <td>CC00302</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.50</td>\n",
       "      <td>59.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1 b 3 CS+ calcerous algae at rear end of shell...</td>\n",
       "      <td>Released</td>\n",
       "      <td>ReleaseSite_50</td>\n",
       "      <td>11/03/02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002_RE_0118</td>\n",
       "      <td>2002-08-08</td>\n",
       "      <td>Researcher_25</td>\n",
       "      <td>CaptureSite_0</td>\n",
       "      <td>Ocean</td>\n",
       "      <td>Beached</td>\n",
       "      <td>Fisher_1918</td>\n",
       "      <td>LandingSite_CaptureSiteCategory_2</td>\n",
       "      <td>Species_5</td>\n",
       "      <td>NotTagged_0113</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.70</td>\n",
       "      <td>33.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>very lively+ right eye is hanging out + swolle...</td>\n",
       "      <td>Released</td>\n",
       "      <td>ReleaseSite_62</td>\n",
       "      <td>08/08/02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.740744Z",
     "start_time": "2024-05-23T22:57:36.738487Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "# Defining function to Standartising column names \n",
    "def standardize_column_names(col):\n",
    "    # Replace spaces with underscores\n",
    "    col = col.replace(' ', '_')\n",
    "    # Insert underscore before each uppercase letter preceded by a lowercase letter or followed by a lowercase letter\n",
    "    col = re.sub(r'(?<=[a-z])(?=[A-Z])', '_', col)\n",
    "    col = re.sub(r'(?<=[A-Z])(?=[A-Z][a-z])', '_', col)\n",
    "    # Convert to lower case\n",
    "    col = col.lower()\n",
    "    # Ensure single underscores only (in case of consecutive underscores from initial spaces)\n",
    "    col = re.sub(r'_+', '_', col)\n",
    "    return col\n"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.767830Z",
     "start_time": "2024-05-23T22:57:36.765255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Applying function to df\n",
    "train_df.columns = [standardize_column_names(col) for col in train_df.columns]\n",
    "\n",
    "# Printing the updated column names to verify the changes\n",
    "print(train_df.columns)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['rescue_id', 'date_time_caught', 'researcher', 'capture_site',\n",
      "       'foraging_ground', 'capture_method', 'fisher', 'landing_site',\n",
      "       'species', 'tag_1', 'tag_2', 'lost_tags', 't_number', 'ccl_cm',\n",
      "       'ccw_cm', 'weight_kg', 'sex', 'turtle_characteristics', 'status',\n",
      "       'release_site', 'date_time_release'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.794067Z",
     "start_time": "2024-05-23T22:57:36.791837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the function to extract the number\n",
    "def extract_number_split(s):\n",
    "    return int(s.split('_')[-1])\n",
    "\n",
    "# Define a function to apply the extraction to multiple columns\n",
    "def apply_extraction(df, columns):\n",
    "    for column in columns:\n",
    "        # Convert column to string type if it's not already\n",
    "        if df[column].dtype != 'object':\n",
    "            df[column] = df[column].astype(str)\n",
    "\n",
    "        # Apply the extraction function\n",
    "        df[column] = df[column].apply(extract_number_split)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.856357Z",
     "start_time": "2024-05-23T22:57:36.834799Z"
    }
   },
   "source": [
    "columns_to_extract_train = ['fisher', 'researcher', 'capture_site', 'species']\n",
    "train_df = apply_extraction(train_df, columns_to_extract_train)"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.888019Z",
     "start_time": "2024-05-23T22:57:36.885297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columns_to_drop = ['rescue_id', 'turtle_characteristics', 'tag_1', 'tag_2', 'lost_tags', 't_number', 'sex',\n",
    "                   'capture_method', 'release_site', 'landing_site', 'status', 'foraging_ground', 'date_time_release']\n",
    "\n",
    "train_df = train_df.drop(columns=columns_to_drop)"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.930091Z",
     "start_time": "2024-05-23T22:57:36.923987Z"
    }
   },
   "cell_type": "code",
   "source": "train_df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      date_time_caught  researcher  capture_site  fisher  species  ccl_cm   \n",
       "0           2000-12-22          25             0    1072        6   64.70  \\\n",
       "1           2001-10-28           6             0     520        6   35.85   \n",
       "2           2001-11-01           6             0    1669        5   51.80   \n",
       "3           2002-03-11          32             0    1798        6   60.50   \n",
       "4           2002-08-08          25             0    1918        5   34.70   \n",
       "...                ...         ...           ...     ...      ...     ...   \n",
       "18057       2018-12-18          30             9     569        5   57.13   \n",
       "18058       2018-12-18          30             9     125        6   42.07   \n",
       "18059       2018-12-24          30             9    1343        5   57.20   \n",
       "18060       2018-12-24          30             9    1551        5   51.90   \n",
       "18061       2018-12-28          30             9    1551        6   34.60   \n",
       "\n",
       "       ccw_cm  weight_kg  \n",
       "0       62.60        NaN  \n",
       "1       31.35        NaN  \n",
       "2       49.20        NaN  \n",
       "3       59.00        NaN  \n",
       "4       33.00        NaN  \n",
       "...       ...        ...  \n",
       "18057   50.57      21.09  \n",
       "18058   38.37       9.02  \n",
       "18059   52.30        NaN  \n",
       "18060   48.50        NaN  \n",
       "18061   31.20       4.29  \n",
       "\n",
       "[18062 rows x 8 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time_caught</th>\n",
       "      <th>researcher</th>\n",
       "      <th>capture_site</th>\n",
       "      <th>fisher</th>\n",
       "      <th>species</th>\n",
       "      <th>ccl_cm</th>\n",
       "      <th>ccw_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-12-22</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1072</td>\n",
       "      <td>6</td>\n",
       "      <td>64.70</td>\n",
       "      <td>62.60</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-10-28</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>6</td>\n",
       "      <td>35.85</td>\n",
       "      <td>31.35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-11-01</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1669</td>\n",
       "      <td>5</td>\n",
       "      <td>51.80</td>\n",
       "      <td>49.20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002-03-11</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>1798</td>\n",
       "      <td>6</td>\n",
       "      <td>60.50</td>\n",
       "      <td>59.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002-08-08</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1918</td>\n",
       "      <td>5</td>\n",
       "      <td>34.70</td>\n",
       "      <td>33.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18057</th>\n",
       "      <td>2018-12-18</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>569</td>\n",
       "      <td>5</td>\n",
       "      <td>57.13</td>\n",
       "      <td>50.57</td>\n",
       "      <td>21.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18058</th>\n",
       "      <td>2018-12-18</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>125</td>\n",
       "      <td>6</td>\n",
       "      <td>42.07</td>\n",
       "      <td>38.37</td>\n",
       "      <td>9.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18059</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>1343</td>\n",
       "      <td>5</td>\n",
       "      <td>57.20</td>\n",
       "      <td>52.30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18060</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>1551</td>\n",
       "      <td>5</td>\n",
       "      <td>51.90</td>\n",
       "      <td>48.50</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18061</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>1551</td>\n",
       "      <td>6</td>\n",
       "      <td>34.60</td>\n",
       "      <td>31.20</td>\n",
       "      <td>4.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18062 rows Ã— 8 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.957962Z",
     "start_time": "2024-05-23T22:57:36.955339Z"
    }
   },
   "source": [
    "def convert_and_split_datetime(df, columns):\n",
    "    \"\"\"\n",
    "    Convert specified datetime columns to timestamp and split into year and week columns\n",
    "    with new names based on the original column names.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the columns.\n",
    "    columns (list): List of column names to convert and split.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with new year and week columns.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        # Convert the column to datetime\n",
    "        df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "\n",
    "        # Extract the base name without 'date_time_' prefix\n",
    "        base_name = column.replace('date_time_', '')\n",
    "\n",
    "        # Create new columns for year and week with the desired names\n",
    "        df[f'year_{base_name}'] = df[column].dt.year\n",
    "        df[f'week_{base_name}'] = df[column].dt.isocalendar().week\n",
    "\n",
    "        # Drop the original datetime column if desired\n",
    "        df.drop(columns=[column], inplace=True)\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:36.999965Z",
     "start_time": "2024-05-23T22:57:36.984048Z"
    }
   },
   "source": [
    "# Apply function to train_df\n",
    "columns_to_convert = ['date_time_caught']\n",
    "train_df = convert_and_split_datetime(train_df, columns_to_convert)\n",
    "\n",
    "train_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   researcher  capture_site  fisher  species  ccl_cm  ccw_cm  weight_kg   \n",
       "0          25             0    1072        6   64.70   62.60        NaN  \\\n",
       "1           6             0     520        6   35.85   31.35        NaN   \n",
       "2           6             0    1669        5   51.80   49.20        NaN   \n",
       "3          32             0    1798        6   60.50   59.00        NaN   \n",
       "4          25             0    1918        5   34.70   33.00        NaN   \n",
       "\n",
       "   year_caught  week_caught  \n",
       "0         2000           51  \n",
       "1         2001           43  \n",
       "2         2001           44  \n",
       "3         2002           11  \n",
       "4         2002           32  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>researcher</th>\n",
       "      <th>capture_site</th>\n",
       "      <th>fisher</th>\n",
       "      <th>species</th>\n",
       "      <th>ccl_cm</th>\n",
       "      <th>ccw_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>year_caught</th>\n",
       "      <th>week_caught</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1072</td>\n",
       "      <td>6</td>\n",
       "      <td>64.70</td>\n",
       "      <td>62.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>6</td>\n",
       "      <td>35.85</td>\n",
       "      <td>31.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1669</td>\n",
       "      <td>5</td>\n",
       "      <td>51.80</td>\n",
       "      <td>49.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>1798</td>\n",
       "      <td>6</td>\n",
       "      <td>60.50</td>\n",
       "      <td>59.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1918</td>\n",
       "      <td>5</td>\n",
       "      <td>34.70</td>\n",
       "      <td>33.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imputing Missing Data in Weight"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:37.013970Z",
     "start_time": "2024-05-23T22:57:37.012096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "def imput_missing_weight_values(df, n = 5):\n",
    "    knn_df = df[['ccl_cm', 'ccw_cm', 'weight_kg']]\n",
    "    imputer = KNNImputer(n_neighbors=n)\n",
    "    imputer.set_output(transform='pandas')\n",
    "\n",
    "    return imputer.fit_transform(knn_df)"
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:44.551101Z",
     "start_time": "2024-05-23T22:57:37.046527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imputed_df = imput_missing_weight_values(train_df)\n",
    "train_df['ccl_cm'] = imputed_df['ccl_cm']\n",
    "train_df['ccw_cm'] = imputed_df['ccw_cm']\n",
    "train_df['weight_kg'] = imputed_df['weight_kg']"
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:44.564490Z",
     "start_time": "2024-05-23T22:57:44.553690Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.info()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18062 entries, 0 to 18061\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   researcher    18062 non-null  int64  \n",
      " 1   capture_site  18062 non-null  int64  \n",
      " 2   fisher        18062 non-null  int64  \n",
      " 3   species       18062 non-null  int64  \n",
      " 4   ccl_cm        18062 non-null  float64\n",
      " 5   ccw_cm        18062 non-null  float64\n",
      " 6   weight_kg     18062 non-null  float64\n",
      " 7   year_caught   18062 non-null  int32  \n",
      " 8   week_caught   18062 non-null  UInt32 \n",
      "dtypes: UInt32(1), float64(3), int32(1), int64(4)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:44.622241Z",
     "start_time": "2024-05-23T22:57:44.612824Z"
    }
   },
   "cell_type": "code",
   "source": "train_df = pd.read_csv('data/train_df.csv')",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Baseline Model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:44.631733Z",
     "start_time": "2024-05-23T22:57:44.622996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_df = train_df.groupby(['year_caught', 'capture_site', 'week_caught']).size().reset_index(name='turtles_rescued')\n",
    "baseline_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      year_caught  capture_site  week_caught  turtles_rescued\n",
       "0            1998            11           28                1\n",
       "1            1998            11           32                1\n",
       "2            1998            11           39                2\n",
       "3            1998            11           43                1\n",
       "4            1998            11           45                1\n",
       "...           ...           ...          ...              ...\n",
       "7952         2018            27           36                1\n",
       "7953         2018            27           38                1\n",
       "7954         2018            27           45                1\n",
       "7955         2018            28           44                1\n",
       "7956         2018            28           45                1\n",
       "\n",
       "[7957 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_caught</th>\n",
       "      <th>capture_site</th>\n",
       "      <th>week_caught</th>\n",
       "      <th>turtles_rescued</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7952</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7953</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7954</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7955</th>\n",
       "      <td>2018</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7956</th>\n",
       "      <td>2018</td>\n",
       "      <td>28</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7957 rows Ã— 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:44.638248Z",
     "start_time": "2024-05-23T22:57:44.632483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_df = baseline_df[~baseline_df['year_caught'].between(1988, 2006)].reset_index(drop=True)\n",
    "baseline_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      year_caught  capture_site  week_caught  turtles_rescued\n",
       "0            2007             2            2                1\n",
       "1            2007             2            3                2\n",
       "2            2007             2            4                1\n",
       "3            2007             2            5                1\n",
       "4            2007             2            7                1\n",
       "...           ...           ...          ...              ...\n",
       "6461         2018            27           36                1\n",
       "6462         2018            27           38                1\n",
       "6463         2018            27           45                1\n",
       "6464         2018            28           44                1\n",
       "6465         2018            28           45                1\n",
       "\n",
       "[6466 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_caught</th>\n",
       "      <th>capture_site</th>\n",
       "      <th>week_caught</th>\n",
       "      <th>turtles_rescued</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6461</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6462</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6463</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6464</th>\n",
       "      <td>2018</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6465</th>\n",
       "      <td>2018</td>\n",
       "      <td>28</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6466 rows Ã— 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:44.640947Z",
     "start_time": "2024-05-23T22:57:44.638984Z"
    }
   },
   "cell_type": "code",
   "source": "baseline_df.drop(['year_caught'], axis=1, inplace=True)",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:44.643116Z",
     "start_time": "2024-05-23T22:57:44.641443Z"
    }
   },
   "cell_type": "code",
   "source": "baseline_test = test_df.copy()",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:44.646304Z",
     "start_time": "2024-05-23T22:57:44.643617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaselinePredictor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def predict_turtles_rescued_all(self):\n",
    "        # Initialize an empty list to store dataframes\n",
    "        dfs = []\n",
    "\n",
    "        # Get unique capture sites and weeks in the baseline_df\n",
    "        capture_site_all = self.df['capture_site'].unique()\n",
    "        weeks_in_year = self.df['week_caught'].unique()\n",
    "\n",
    "        # Iterate over each capture site and week to calculate the mean turtles_rescued\n",
    "        for capture_site in capture_site_all:\n",
    "            for week in weeks_in_year:\n",
    "                # Calculate mean turtles_rescued for the current capture site and week\n",
    "                mean_turtles_rescued = self.df[(self.df['capture_site'] == capture_site) & (self.df['week_caught'] == week)]['turtles_rescued'].mean()\n",
    "\n",
    "                # Append a dataframe to the list\n",
    "                dfs.append(pd.DataFrame({'capture_site': [capture_site], 'week_caught': [week], 'turtles_rescued': [mean_turtles_rescued]}))\n",
    "\n",
    "        # Concatenate all dataframes in the list\n",
    "        predict_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        return predict_df"
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:45.154915Z",
     "start_time": "2024-05-23T22:57:44.648292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the predictor with the baseline_df\n",
    "predictor = BaselinePredictor(baseline_df)\n",
    "\n",
    "# Predict the baseline values\n",
    "predict_baseline = predictor.predict_turtles_rescued_all()\n",
    "\n",
    "# Print the predicted baseline DataFrame\n",
    "print(predict_baseline)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      capture_site  week_caught  turtles_rescued\n",
      "0                2            2         1.666667\n",
      "1                2            3         1.333333\n",
      "2                2            4         1.000000\n",
      "3                2            5         1.500000\n",
      "4                2            7         1.000000\n",
      "...            ...          ...              ...\n",
      "1532            17           37         2.333333\n",
      "1533            17           39         1.000000\n",
      "1534            17           33         1.250000\n",
      "1535            17           20         1.000000\n",
      "1536            17           53         1.000000\n",
      "\n",
      "[1537 rows x 3 columns]\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RMSE Baseline"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:36:30.207333Z",
     "start_time": "2024-05-24T09:36:30.164296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(test_df['turtles_rescued'].describe())\n",
    "\n",
    "# Baseline RMSE\n",
    "mean_turtles_rescued = baseline_test['turtles_rescued'].mean()\n",
    "baseline_predictions = [mean_turtles_rescued] * len(test_df)\n",
    "baseline_mse = mean_squared_error(baseline_test['turtles_rescued'], baseline_predictions)\n",
    "baseline_rmse = np.sqrt(baseline_mse)\n",
    "print(f\"Baseline RMSE: {baseline_rmse}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 3.9440045387035063\n",
      "count    1276.000000\n",
      "mean        4.436520\n",
      "std         2.878706\n",
      "min         0.000000\n",
      "25%         2.000000\n",
      "50%         4.000000\n",
      "75%         7.000000\n",
      "max         9.000000\n",
      "Name: turtles_rescued, dtype: float64\n",
      "Baseline RMSE: 2.8775776437795377\n"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Interactive version"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:45.177448Z",
     "start_time": "2024-05-23T22:57:45.175679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_turtles_rescued(capture_site, week):\n",
    "    predict_df = baseline_df[(baseline_df['capture_site'] == 5) & (baseline_df['week_caught'] == 5)]\n",
    "    turtle_rescued = predict_df['turtles_rescued'].mean()\n",
    "    return turtle_rescued"
   ],
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:48.177473Z",
     "start_time": "2024-05-23T22:57:45.178149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "week = input(f'Enter a week number for which you would like to predict (from 1 to {len(baseline_df.week_caught.unique())}):')\n",
    "capture_site = input(f'Enter a capture site number (from 1 to 29):')\n",
    "print(f'Predicted Turtles Rescued for {capture_site} and {week}: {predict_turtles_rescued(capture_site, week)}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Turtles Rescued for  and : 1.0\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Enseble Model Staking"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transform data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:48.182326Z",
     "start_time": "2024-05-23T22:57:48.179350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:48.189256Z",
     "start_time": "2024-05-23T22:57:48.183559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "to_transform_train = train_df.groupby(['year_caught', 'capture_site', 'week_caught']).size().reset_index(name='turtles_rescued')\n",
    "to_transform_test = test_df.copy()"
   ],
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:57:48.197186Z",
     "start_time": "2024-05-23T22:57:48.190256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TurtleRescueModifierTrain:\n",
    "    def __init__(self):\n",
    "        # Generate arrays for year, week_caught, and capture_site\n",
    "        years = np.repeat(np.arange(1988, 2019), 53 * 29)\n",
    "        week_caught = np.tile(np.arange(1, 54), 31 * 29)\n",
    "        capture_site = np.repeat(np.arange(1, 30), 53 * 31)\n",
    "\n",
    "        # Create the DataFrame\n",
    "        self.df = pd.DataFrame({\n",
    "            'year_caught': years,\n",
    "            'week_caught': week_caught,\n",
    "            'capture_site': capture_site,\n",
    "            'turtles_rescued': np.zeros(len(years))\n",
    "        })\n",
    "\n",
    "    def merge_data(self, source_df):\n",
    "        for index, row in source_df.iterrows():\n",
    "            # Match conditions based on year_caught, week_caught, and capture_site\n",
    "            match_condition = (\n",
    "                    (self.df['year_caught'] == row['year_caught']) &\n",
    "                    (self.df['week_caught'] == row['week_caught']) &\n",
    "                    (self.df['capture_site'] == row['capture_site'])\n",
    "            )\n",
    "\n",
    "            # Check if a matching row exists in the target_df\n",
    "            matching_row_index = self.df.index[match_condition]\n",
    "\n",
    "            if len(matching_row_index) > 0:\n",
    "                # Update the existing row in target_df with data from source_df\n",
    "                self.df.loc[matching_row_index[0], 'turtles_rescued'] = row['turtles_rescued']\n",
    "            else:\n",
    "                # If no matching row exists, create a new row in target_df\n",
    "                new_row = {\n",
    "                    'year_caught': row['year_caught'],\n",
    "                    'week_caught': row['week_caught'],\n",
    "                    'capture_site': row['capture_site'],\n",
    "                    'turtles_rescued': row['turtles_rescued']\n",
    "                }\n",
    "                self.df = pd.concat([self.df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def impute_missing_values(self):\n",
    "        \"\"\"\n",
    "        Impute missing values in the 'turtles_rescued' column using K-Nearest Neighbors (KNN).\n",
    "        \"\"\"\n",
    "        # Ensure some 'turtles_rescued' values are NaN for imputation demonstration\n",
    "        self.df.loc[self.df.sample(frac=0.1).index, 'turtles_rescued'] = np.nan\n",
    "\n",
    "        # Select columns for imputation\n",
    "        features = self.df[['year_caught', 'week_caught', 'capture_site']]\n",
    "        targets = self.df[['turtles_rescued']]\n",
    "\n",
    "        # Combine features and targets for imputation\n",
    "        combined = pd.concat([features, targets], axis=1)\n",
    "\n",
    "        # Initialize KNN Imputer with k=5\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "        # Impute missing values\n",
    "        imputed_data = imputer.fit_transform(combined)\n",
    "        self.df['turtles_rescued'] = imputed_data[:, -1]\n",
    "        \n",
    "        # Convert 'turtles_rescued' to integer type\n",
    "        self.df['turtles_rescued'] = self.df['turtles_rescued'].astype(int)\n",
    "        return self.df\n"
   ],
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:58:03.159128Z",
     "start_time": "2024-05-23T22:57:48.198112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "merged_train = TurtleRescueModifierTrain()\n",
    "merged_train.merge_data(to_transform_train)\n",
    "imputed_train = TurtleRescueModifierTrain.impute_missing_values(merged_train)"
   ],
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:58:03.167592Z",
     "start_time": "2024-05-23T22:58:03.160829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TurtleRescueModifierTest:\n",
    "    def __init__(self, year):\n",
    "        \"\"\"\n",
    "        Initialize the TurtleRescueModifierTrain with data for a specific year.\n",
    "        \n",
    "        :param year: The specific year for which to initialize the data.\n",
    "        \"\"\"\n",
    "        # Generate arrays for the specified year, week_caught, and capture_site\n",
    "        self.year = year\n",
    "        weeks_per_year = 53\n",
    "        capture_sites = 29\n",
    "\n",
    "        year = np.repeat(year, weeks_per_year * capture_sites)\n",
    "        week_caught = np.tile(np.arange(1, weeks_per_year + 1), capture_sites)\n",
    "        capture_site = np.repeat(np.arange(1, capture_sites + 1), weeks_per_year)\n",
    "\n",
    "        # Create the DataFrame\n",
    "        self.df = pd.DataFrame({\n",
    "            'year_caught': year,\n",
    "            'week_caught': week_caught,\n",
    "            'capture_site': capture_site,\n",
    "            'turtles_rescued': np.zeros(len(year))\n",
    "        })\n",
    "\n",
    "    def merge_data(self, source_df):\n",
    "        # Add 'year_caught' column to source_df if it doesn't exist\n",
    "        if 'year_caught' not in source_df.columns:\n",
    "            source_df['year_caught'] = self.year\n",
    "        \n",
    "        for index, row in source_df.iterrows():\n",
    "            # Match conditions based on year_caught, week_caught, and capture_site\n",
    "            match_condition = (\n",
    "                (self.df['year_caught'] == row['year_caught']) &\n",
    "                (self.df['week_caught'] == row['week_caught']) &\n",
    "                (self.df['capture_site'] == row['capture_site'])\n",
    "            )\n",
    "\n",
    "            # Check if a matching row exists in the target_df\n",
    "            matching_row_index = self.df.index[match_condition]\n",
    "\n",
    "            if len(matching_row_index) > 0:\n",
    "                # Update the existing row in target_df with data from source_df\n",
    "                self.df.loc[matching_row_index[0], 'turtles_rescued'] = row['turtles_rescued']\n",
    "            else:\n",
    "                # If no matching row exists, create a new row in target_df\n",
    "                new_row = {\n",
    "                    'year_caught': row['year_caught'],\n",
    "                    'week_caught': row['week_caught'],\n",
    "                    'capture_site': row['capture_site'],\n",
    "                    'turtles_rescued': row['turtles_rescued']\n",
    "                }\n",
    "                self.df = pd.concat([self.df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "    def impute_missing_values(self):\n",
    "        \"\"\"\n",
    "        Impute missing values in the 'turtles_rescued' column using K-Nearest Neighbors (KNN).\n",
    "        \"\"\"\n",
    "        # Introduce NaN values into the 'turtles_rescued' column to demonstrate imputation\n",
    "        self.df.loc[self.df.sample(frac=0.1).index, 'turtles_rescued'] = np.nan\n",
    "\n",
    "        # Initialize KNN Imputer with k=5\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "        # Select columns for imputation\n",
    "        features = self.df[['year_caught', 'week_caught', 'capture_site']]\n",
    "        targets = self.df[['turtles_rescued']]\n",
    "\n",
    "        # Combine features and targets for imputation\n",
    "        combined = pd.concat([features, targets], axis=1)\n",
    "\n",
    "        # Impute missing values\n",
    "        imputed_data = imputer.fit_transform(combined)\n",
    "        self.df['turtles_rescued'] = imputed_data[:, -1]\n",
    "\n",
    "        # Convert 'turtles_rescued' to integer type\n",
    "        self.df['turtles_rescued'] = self.df['turtles_rescued'].astype(int)\n",
    "\n",
    "        return self.df\n"
   ],
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:58:03.456598Z",
     "start_time": "2024-05-23T22:58:03.168427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "merged_test = TurtleRescueModifierTest(year = 2019)\n",
    "merged_test.merge_data(to_transform_test)\n",
    "imputed_test = merged_test.impute_missing_values()"
   ],
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T22:58:03.486584Z",
     "start_time": "2024-05-23T22:58:03.471090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TurtleRescueContainer:\n",
    "    def __init__(self):\n",
    "        # Initialize the DataFrame with 53 weeks and 29 capture sites\n",
    "        self.df = pd.DataFrame({\n",
    "            'week_caught': np.tile(np.arange(1, 54), 29),\n",
    "            'capture_site': np.repeat(np.arange(1, 30), 53),\n",
    "            'turtles_rescued': np.zeros(53 * 29),\n",
    "            'weight_week': np.zeros(53 * 29),\n",
    "            'weight_capture_site': np.zeros(53 * 29)\n",
    "        })\n",
    "        self.df['weight_combined'] = np.zeros(53 * 29)\n",
    "\n",
    "    def update_with_mean(self, df1, df2):\n",
    "        \"\"\"\n",
    "        Update the internal DataFrame with means of turtles_rescued, weight_week,\n",
    "        and weight_capture_site from two DataFrames grouped by week_caught and capture_site.\n",
    "\n",
    "        Parameters:\n",
    "        df1 (pd.DataFrame): The DataFrame containing 'week_caught', 'turtles_rescued', 'weight_week'.\n",
    "        df2 (pd.DataFrame): The DataFrame containing 'capture_site', 'turtles_rescued', 'weight_capture_site'.\n",
    "        \"\"\"\n",
    "        # Calculate mean values for df1 grouped by week_caught\n",
    "        if 'week_caught' in df1.columns:\n",
    "            df1_grouped = df1.groupby('week_caught').agg({\n",
    "                'turtles_rescued': 'mean',\n",
    "                'weight_week': 'mean'\n",
    "            }).reset_index()\n",
    "\n",
    "            # Merge the means into the internal DataFrame\n",
    "            self.df = self.df.merge(df1_grouped, on='week_caught', how='left', suffixes=('', '_df1'))\n",
    "            self.df['turtles_rescued'] = self.df[['turtles_rescued', 'turtles_rescued_df1']].mean(axis=1)\n",
    "            self.df['weight_week'] = self.df[['weight_week', 'weight_week_df1']].mean(axis=1)\n",
    "            self.df.drop(columns=['turtles_rescued_df1', 'weight_week_df1'], inplace=True)\n",
    "\n",
    "        # Calculate mean values for df2 grouped by capture_site\n",
    "        if 'capture_site' in df2.columns:\n",
    "            df2_grouped = df2.groupby('capture_site').agg({\n",
    "                'turtles_rescued': 'mean',\n",
    "                'weight_capture_site': 'mean'\n",
    "            }).reset_index()\n",
    "\n",
    "            # Merge the means into the internal DataFrame\n",
    "            self.df = self.df.merge(df2_grouped, on='capture_site', how='left', suffixes=('', '_df2'))\n",
    "            self.df['turtles_rescued'] = self.df[['turtles_rescued', 'turtles_rescued_df2']].mean(axis=1)\n",
    "            self.df['weight_capture_site'] = self.df[['weight_capture_site', 'weight_capture_site_df2']].mean(axis=1)\n",
    "            self.df.drop(columns=['turtles_rescued_df2', 'weight_capture_site_df2'], inplace=True)\n",
    "\n",
    "        # Calculate the combined weight\n",
    "        self.df['weight_combined'] = self.df['weight_week'] + self.df['weight_capture_site']\n",
    "        \n",
    "        def impute_missing(self):\n",
    "         \"\"\"\n",
    "         Impute missing values in the 'turtles_rescued' column using K-Nearest Neighbors (KNN).\n",
    "         \"\"\"\n",
    "         # Extract features for imputation\n",
    "         features = self.df[['year', 'week_caught', 'capture_site']]\n",
    "        \n",
    "         # Initialize KNN Imputer with k=5 (you can adjust k as needed)\n",
    "         imputer = KNNImputer(n_neighbors=5)\n",
    "        \n",
    "         # Impute missing values\n",
    "         self.df['turtles_rescued'] = imputer.fit_transform(features)\n",
    "\n",
    "        \n",
    "        return self.df"
   ],
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simple ML model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# loading data for model\n",
    "train_simple = imputed_train.copy()\n",
    "test_simple = imputed_test.copy()\n",
    "\n",
    "# Extract features and target from train_week DataFrame\n",
    "X_train_simple = train_simple[['year_caught', 'week_caught', 'capture_site']]\n",
    "y_train_simple = train_simple['turtles_rescued']\n",
    "\n",
    "X_test_simple = test_simple[['year_caught', 'week_caught', 'capture_site']]\n",
    "y_test_simple = ['turtles_rescued']\n",
    "\n",
    "# Define a list of regression models\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(),\n",
    "    Lasso(),\n",
    "    DecisionTreeRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    SVR()\n",
    "]\n",
    "\n",
    "# Define a list to store the trained models\n",
    "trained_models_simple = []\n",
    "\n",
    "# Iterate over each model and fit it to the training data\n",
    "for model in models:\n",
    "    model.fit(X_train_simple, y_train_simple)\n",
    "    trained_models_simple.append(model)\n",
    "\n",
    "# Perform cross-validation for each model and evaluate their performance\n",
    "for model in models:\n",
    "    scores_week_caught = cross_val_score(model, X_train_simple, y_train_simple, scoring='neg_mean_squared_error', cv=5)\n",
    "    rmse_scores_week_caught = np.sqrt(-scores_week_caught)\n",
    "    print(f\"{model.__class__.__name__}: Mean RMSE: {rmse_scores_week_caught.mean()}, Std RMSE: {rmse_scores_week_caught.std()}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ensemble model "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:32:20.136207Z",
     "start_time": "2024-05-24T09:29:16.306190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define a list of regression models\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(),\n",
    "    Lasso(),\n",
    "    DecisionTreeRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    SVR()\n",
    "]\n",
    "\n",
    "# Train and evaluate each model\n",
    "trained_models_simple = []\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models_simple.append(model)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores_simple = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "    rmse_scores_simple = np.sqrt(-scores_simple)\n",
    "    print(f\"{model.__class__.__name__}: Mean RMSE (CV): {rmse_scores_simple.mean()}, Std RMSE (CV): {rmse_scores_simple.std()}\")\n",
    "\n",
    "    # Calculate RMSE on the test set\n",
    "    predictions_test = model.predict(X_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, predictions_test))\n",
    "    print(f\"{model.__class__.__name__}: RMSE (Test): {rmse_test}\")\n",
    "\n",
    "# Define the base estimators for stacking\n",
    "estimators = [\n",
    "    ('ridge', Ridge(alpha=10)),\n",
    "    ('lasso', Lasso(alpha=0.1)),\n",
    "    ('gbr', GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=5))\n",
    "]\n",
    "\n",
    "# Create the stacking regressor with Linear Regression as the final estimator\n",
    "final_estimator = LinearRegression()\n",
    "stacking_regressor = StackingRegressor(estimators=estimators, final_estimator=final_estimator)\n",
    "\n",
    "# Train the stacking regressor\n",
    "stacking_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the stacking regressor on the test set\n",
    "stacking_predictions = stacking_regressor.predict(X_test)\n",
    "stacking_rmse = np.sqrt(mean_squared_error(y_test, stacking_predictions))\n",
    "print(f\"Stacking Model RMSE (Test): {stacking_rmse}\")\n",
    "\n",
    "# Perform cross-validation for the stacking model\n",
    "stacking_cv_scores = cross_val_score(stacking_regressor, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "stacking_cv_rmse = np.sqrt(-stacking_cv_scores)\n",
    "print(f\"Stacking Model Mean RMSE (CV): {stacking_cv_rmse.mean()}, Std RMSE (CV): {stacking_cv_rmse.std()}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression: Mean RMSE (CV): 1.1423161391458305, Std RMSE (CV): 0.053385205539318116\n",
      "LinearRegression: RMSE (Test): 1.1438915753380086\n",
      "Ridge: Mean RMSE (CV): 1.1423161384208917, Std RMSE (CV): 0.053385213596612496\n",
      "Ridge: RMSE (Test): 1.143891573906307\n",
      "Lasso: Mean RMSE (CV): 1.200933182501268, Std RMSE (CV): 0.05879184303830722\n",
      "Lasso: RMSE (Test): 1.2018821186165782\n",
      "DecisionTreeRegressor: Mean RMSE (CV): 1.0912595546037704, Std RMSE (CV): 0.0614285750713496\n",
      "DecisionTreeRegressor: RMSE (Test): 1.1020641503934943\n",
      "RandomForestRegressor: Mean RMSE (CV): 0.8656135264110377, Std RMSE (CV): 0.057189134479072465\n",
      "RandomForestRegressor: RMSE (Test): 0.8882171233162441\n",
      "SVR: Mean RMSE (CV): 1.3201629784585647, Std RMSE (CV): 0.06265591337407421\n",
      "SVR: RMSE (Test): 1.3106863473952135\n",
      "Stacking Model RMSE (Test): 0.8249750684273498\n",
      "Stacking Model Mean RMSE (CV): 0.8138215010045841, Std RMSE (CV): 0.052474907402054384\n"
     ]
    }
   ],
   "execution_count": 151
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
