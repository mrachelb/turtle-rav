{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "capture_site = pd.read_csv('data/CaptureSite_category.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "capture_site"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sample_sub formating"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:42:38.881221Z",
     "start_time": "2024-05-23T10:42:38.849855Z"
    }
   },
   "source": [
    "sample_sub = pd.read_csv('data/Sample_sub.csv')"
   ],
   "outputs": [],
   "execution_count": 335
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample_sub.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:42:43.435996Z",
     "start_time": "2024-05-23T10:42:43.427444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_capture_site(df):\n",
    "    df['capture_site'] = df['ID'].apply(lambda x: x.split('_')[-2])\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 336
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:42:43.919731Z",
     "start_time": "2024-05-23T10:42:43.916063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_and_convert_week(df):\n",
    "    # Extract the second to last element\n",
    "    df['week_caught'] = df['ID'].apply(lambda x: x[-2:])\n",
    "\n",
    "    # Convert to datetime with appropriate format for year and month (\"%Y%m\")\n",
    "    df['week_caught'] = df['week_caught'].apply(lambda x : int(x))\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 337
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:42:45.207372Z",
     "start_time": "2024-05-23T10:42:45.200701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def formating_sample_sub(df): \n",
    "    # extracting capture site id\n",
    "    df = extract_capture_site(df)\n",
    "    \n",
    "    # extracting week of rascue\n",
    "    df = extract_and_convert_week(df)\n",
    "    \n",
    "    # renaming columns to match training set\n",
    "    df.rename(columns={'Capture_Number': 'turtles_rescued'}, inplace=True)\n",
    "    \n",
    "    # getting rid of mixed column\n",
    "    df.drop(columns=['ID'], inplace=True)\n",
    "    \n",
    "    # Standartising prediction \n",
    "    df = df.groupby(['capture_site', 'week_caught'])['turtles_rescued'].sum().reset_index()\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 338
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:42:46.112134Z",
     "start_time": "2024-05-23T10:42:46.080220Z"
    }
   },
   "cell_type": "code",
   "source": "sample_sub = formating_sample_sub(sample_sub)",
   "outputs": [],
   "execution_count": 339
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train_df formating"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_df = pd.read_csv('data/train.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "train_df.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Standartising column names \n",
    "def standardize_column_names(col):\n",
    "    # Replace spaces with underscores\n",
    "    col = col.replace(' ', '_')\n",
    "    # Insert underscore before each uppercase letter preceded by a lowercase letter or followed by a lowercase letter\n",
    "    col = re.sub(r'(?<=[a-z])(?=[A-Z])', '_', col)\n",
    "    col = re.sub(r'(?<=[A-Z])(?=[A-Z][a-z])', '_', col)\n",
    "    # Convert to lower case\n",
    "    col = col.lower()\n",
    "    # Ensure single underscores only (in case of consecutive underscores from initial spaces)\n",
    "    col = re.sub(r'_+', '_', col)\n",
    "    return col\n",
    "\n",
    "train_df.columns = [standardize_column_names(col) for col in train_df.columns]\n",
    "\n",
    "# Printing the updated column names to verify the changes\n",
    "print(train_df.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#train_df.groupby('ReleaseSite').size()\n",
    "\n",
    "#We can drop the column Sex, since the most values are Unknown \n",
    "df = train_df.groupby('date_time_caught').size().reset_index(name='Size')\n",
    "df.sort_values('Size', ascending= False).tail(60)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def extract_number_split(s):\n",
    "    num = s.split('_')[-1]\n",
    "    return int(num)\n",
    "\n",
    "extract_number_split('Fischer_5')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_df['fisher'] = train_df['fisher'].apply(extract_number_split)\n",
    "\n",
    "train_df['fisher']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_df['researcher'] = train_df['researcher'].apply(extract_number_split)\n",
    "\n",
    "train_df['researcher']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_df['capture_site'] = train_df['capture_site'].apply(extract_number_split)\n",
    "\n",
    "train_df['capture_site']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "train_df['species'] = train_df['species'].apply(extract_number_split)\n",
    "\n",
    "train_df['species']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "#train_df['ReleaseSite'] = train_df['ReleaseSite'].apply(extract_number_split)\n",
    "\n",
    "#train_df['ReleaseSite']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_and_split_datetime(df, columns):\n",
    "    \"\"\"\n",
    "    Convert specified datetime columns to timestamp and split into year and week columns\n",
    "    with new names based on the original column names.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the columns.\n",
    "    columns (list): List of column names to convert and split.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with new year and week columns.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        # Convert the column to datetime\n",
    "        df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "\n",
    "        # Extract the base name without 'date_time_' prefix\n",
    "        base_name = column.replace('date_time_', '')\n",
    "\n",
    "        # Create new columns for year and week with the desired names\n",
    "        df[f'year_{base_name}'] = df[column].dt.year\n",
    "        df[f'week_{base_name}'] = df[column].dt.isocalendar().week\n",
    "\n",
    "        # Drop the original datetime column if desired\n",
    "        df.drop(columns=[column], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'df' is your DataFrame\n",
    "columns_to_convert = ['date_time_caught', 'date_time_release']\n",
    "train_df = convert_and_split_datetime(train_df, columns_to_convert)\n",
    "\n",
    "train_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "category_counts = train_df['week_caught'].value_counts()\n",
    "\n",
    "# Plotting using seaborn\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x=category_counts.index, y=category_counts.values, palette='viridis')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('week')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Categories')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "category_counts = train_df['capture_site'].value_counts()\n",
    "\n",
    "# Plotting using seaborn\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x=category_counts.index, y=category_counts.values, palette='viridis')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('capture_site')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Categories')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_df.nunique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = train_df.groupby('status').size().reset_index(name='size')\n",
    "df.sort_values('size', ascending= False).head(60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#We want to start the \n",
    "\n",
    "train_df.query('Date_TimeRelease.isna()')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_df[['Lost_Tags', 'Tag_1', 'Tag_2','Rescue_ID' ,'Date_TimeCaught']].query('Tag_1.isna() and Tag_2.isna() and Lost_Tags.isna()')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Change the fisher to just a fisher ID, Same for Researcher\n",
    "\n",
    "df = train_df.groupby('Researcher').size().reset_index(name='Size')\n",
    "df.sort_values('Size', ascending= False).head(60)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build in a function using apply to convert the date to Week calender"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#We can drop the column Sex, since the most values are Unknown \n",
    "df = train_df.groupby('Sex').size().reset_index(name='Size')\n",
    "df.sort_values('Size', ascending= False).head(60)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Baseline Model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:41:06.882947Z",
     "start_time": "2024-05-22T18:41:06.852022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_df = train_df.groupby(['year_caught', 'capture_site', 'week_caught']).size().reset_index(name='turtles_rescued')\n",
    "baseline_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      year_caught  capture_site  week_caught  turtles_rescued\n",
       "0            1998            11           28                1\n",
       "1            1998            11           32                1\n",
       "2            1998            11           39                2\n",
       "3            1998            11           43                1\n",
       "4            1998            11           45                1\n",
       "...           ...           ...          ...              ...\n",
       "7952         2018            27           36                1\n",
       "7953         2018            27           38                1\n",
       "7954         2018            27           45                1\n",
       "7955         2018            28           44                1\n",
       "7956         2018            28           45                1\n",
       "\n",
       "[7957 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_caught</th>\n",
       "      <th>capture_site</th>\n",
       "      <th>week_caught</th>\n",
       "      <th>turtles_rescued</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7952</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7953</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7954</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7955</th>\n",
       "      <td>2018</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7956</th>\n",
       "      <td>2018</td>\n",
       "      <td>28</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7957 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 317
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:41:10.588987Z",
     "start_time": "2024-05-22T18:41:10.577547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_df = baseline_df[~baseline_df['year_caught'].between(1988, 2006)].reset_index(drop=True)\n",
    "baseline_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      year_caught  capture_site  week_caught  turtles_rescued\n",
       "0            2007             2            2                1\n",
       "1            2007             2            3                2\n",
       "2            2007             2            4                1\n",
       "3            2007             2            5                1\n",
       "4            2007             2            7                1\n",
       "...           ...           ...          ...              ...\n",
       "6461         2018            27           36                1\n",
       "6462         2018            27           38                1\n",
       "6463         2018            27           45                1\n",
       "6464         2018            28           44                1\n",
       "6465         2018            28           45                1\n",
       "\n",
       "[6466 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_caught</th>\n",
       "      <th>capture_site</th>\n",
       "      <th>week_caught</th>\n",
       "      <th>turtles_rescued</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6461</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6462</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6463</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6464</th>\n",
       "      <td>2018</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6465</th>\n",
       "      <td>2018</td>\n",
       "      <td>28</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6466 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 318
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:41:13.162677Z",
     "start_time": "2024-05-22T18:41:13.157521Z"
    }
   },
   "cell_type": "code",
   "source": "baseline_df.drop(['year_caught'], axis=1, inplace=True)",
   "outputs": [],
   "execution_count": 319
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:42:11.076567Z",
     "start_time": "2024-05-23T10:42:11.047734Z"
    }
   },
   "cell_type": "code",
   "source": "baseline_test = sample_sub.copy()",
   "outputs": [],
   "execution_count": 334
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:33:03.312654Z",
     "start_time": "2024-05-23T10:33:03.261826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaselinePredictor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def predict_turtles_rescued_all(self):\n",
    "        # Initialize an empty list to store dataframes\n",
    "        dfs = []\n",
    "\n",
    "        # Get unique capture sites and weeks in the baseline_df\n",
    "        capture_site_all = self.df['capture_site'].unique()\n",
    "        weeks_in_year = self.df['week_caught'].unique()\n",
    "\n",
    "        # Iterate over each capture site and week to calculate the mean turtles_rescued\n",
    "        for capture_site in capture_site_all:\n",
    "            for week in weeks_in_year:\n",
    "                # Calculate mean turtles_rescued for the current capture site and week\n",
    "                mean_turtles_rescued = self.df[(self.df['capture_site'] == capture_site) & (self.df['week_caught'] == week)]['turtles_rescued'].mean()\n",
    "\n",
    "                # Append a dataframe to the list\n",
    "                dfs.append(pd.DataFrame({'capture_site': [capture_site], 'week_caught': [week], 'turtles_rescued': [mean_turtles_rescued]}))\n",
    "\n",
    "        # Concatenate all dataframes in the list\n",
    "        predict_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        return predict_df"
   ],
   "outputs": [],
   "execution_count": 329
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:33:10.299303Z",
     "start_time": "2024-05-23T10:33:09.409572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the predictor with the baseline_df\n",
    "predictor = BaselinePredictor(baseline_df)\n",
    "\n",
    "# Predict the baseline values\n",
    "predict_baseline = predictor.predict_turtles_rescued_all()\n",
    "\n",
    "# Print the predicted baseline DataFrame\n",
    "print(predict_baseline)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      capture_site  week_caught  turtles_rescued\n",
      "0                2            2         1.666667\n",
      "1                2            3         1.333333\n",
      "2                2            4         1.000000\n",
      "3                2            5         1.500000\n",
      "4                2            7         1.000000\n",
      "...            ...          ...              ...\n",
      "1532            17           37         2.333333\n",
      "1533            17           39         1.000000\n",
      "1534            17           33         1.250000\n",
      "1535            17           20         1.000000\n",
      "1536            17           53         1.000000\n",
      "\n",
      "[1537 rows x 3 columns]\n"
     ]
    }
   ],
   "execution_count": 330
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:44:15.808104Z",
     "start_time": "2024-05-23T10:44:15.765011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Ensure both DataFrames have the same number of rows\n",
    "num_rows_ = baseline_test.shape[0]\n",
    "\n",
    "# Randomly sample rows from predict_baseline to match the number of rows in sample_sub\n",
    "predict_baseline_trimmed = predict_baseline.sample(n=num_rows_sample_sub, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Ensure the indices match\n",
    "baseline_test = baseline_test.reset_index(drop=True)\n",
    "\n",
    "# Combine both DataFrames to ensure we drop NaNs in corresponding rows\n",
    "combined_df = pd.concat([baseline_test['turtles_rescued'], predict_baseline_trimmed['turtles_rescued']], axis=1, keys=['true', 'pred'])\n",
    "\n",
    "# Drop rows with NaN values in either column\n",
    "combined_df = combined_df.dropna()\n",
    "\n",
    "# Separate the true and predicted values\n",
    "y_true = combined_df['true']\n",
    "y_pred = combined_df['pred']\n",
    "\n",
    "# Calculate MAE\n",
    "mae_baseline = mean_absolute_error(y_true, y_pred)\n",
    "print(mae_baseline)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2108299243509903\n"
     ]
    }
   ],
   "execution_count": 340
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Contextualising MAE"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:33:44.626389Z",
     "start_time": "2024-05-23T10:33:44.610908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(baseline_test.turtles_rescued.min())\n",
    "print(baseline_test.turtles_rescued.max())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "9\n"
     ]
    }
   ],
   "execution_count": 331
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:33:49.338193Z",
     "start_time": "2024-05-23T10:33:49.315886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_min = 1\n",
    "target_max = 9\n",
    "target_range = target_max - target_min\n",
    "acceptable_mae = target_range * 0.1  # Example threshold of 10% of the range\n",
    "print(f\"Acceptable MAE: {acceptable_mae}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceptable MAE: 0.8\n"
     ]
    }
   ],
   "execution_count": 332
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RMSE Baseline"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:46:41.201395Z",
     "start_time": "2024-05-23T10:46:41.170267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(sample_sub['turtles_rescued'].describe())\n",
    "\n",
    "# Baseline RMSE\n",
    "mean_turtles_rescued = baseline_test['turtles_rescued'].mean()\n",
    "baseline_predictions = [mean_turtles_rescued] * len(sample_sub)\n",
    "baseline_mse = mean_squared_error(baseline_test['turtles_rescued'], baseline_predictions)\n",
    "baseline_rmse = np.sqrt(baseline_mse)\n",
    "print(f\"Baseline RMSE: {baseline_rmse}\")\n",
    "\n",
    "# Coefficient of Variation of RMSE\n",
    "cv_rmse = (rmse / mean_turtles_rescued) * 100\n",
    "print(f\"Coefficient of Variation of RMSE: {cv_rmse:.2f}%\")\n",
    "\n",
    "# Standard Deviation of Turtles Rescued\n",
    "std_turtles_rescued = baseline_test['turtles_rescued'].std()\n",
    "print(f\"Standard Deviation of Turtles Rescued: {std_turtles_rescued}\")\n",
    "print(f\"RMSE as a proportion of Standard Deviation: {rmse / std_turtles_rescued}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 3.9440045387035063\n",
      "count    1276.000000\n",
      "mean        4.436520\n",
      "std         2.878706\n",
      "min         0.000000\n",
      "25%         2.000000\n",
      "50%         4.000000\n",
      "75%         7.000000\n",
      "max         9.000000\n",
      "Name: turtles_rescued, dtype: float64\n",
      "Baseline RMSE: 2.8775776437795377\n",
      "Coefficient of Variation of RMSE: 88.90%\n",
      "Standard Deviation of Turtles Rescued: 2.878705884420333\n",
      "RMSE as a proportion of Standard Deviation: 1.370061651677794\n"
     ]
    }
   ],
   "execution_count": 341
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Benchmarking against baseline"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T10:46:55.825671Z",
     "start_time": "2024-05-23T10:46:55.799557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the MAE for the baseline model\n",
    "baseline_mae = mean_absolute_error(sample_sub['turtles_rescued'], baseline_predictions)\n",
    "print(f\"Baseline MAE: {baseline_mae}\")\n",
    "\n",
    "# Compare baseline MAE with your model's MAE\n",
    "if mae_baseline < baseline_mae:\n",
    "    print(\"Your model is performing better than the baseline.\")\n",
    "else:\n",
    "    print(\"Your model is not performing better than the baseline.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE: 2.519082703589784\n",
      "Your model is not performing better than the baseline.\n"
     ]
    }
   ],
   "execution_count": 342
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Interactive version"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_turtles_rescued(capture_site, week):\n",
    "    predict_df = baseline_df[(baseline_df['capture_site'] == 5) & (baseline_df['week_caught'] == 5)]\n",
    "    turtle_rescued = predict_df['turtles_rescued'].mean()\n",
    "    return turtle_rescued"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "week = input(f'Enter a week number for which you would like to predict (from 1 to {len(baseline_df.week_caught.unique())}):')\n",
    "capture_site = input(f'Enter a capture site number (from 1 to 29):')\n",
    "print(f'Predicted Turtles Rescued for {capture_site} and {week}: {predict_turtles_rescued(capture_site, week)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Linear Regression"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:47:56.883818Z",
     "start_time": "2024-05-22T18:47:56.872783Z"
    }
   },
   "cell_type": "code",
   "source": "import sklearn.linear_model ",
   "outputs": [],
   "execution_count": 325
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:47:57.459048Z",
     "start_time": "2024-05-22T18:47:57.456718Z"
    }
   },
   "cell_type": "code",
   "source": "linear_regression = linear_model.LinearRegression()",
   "outputs": [],
   "execution_count": 326
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:48:28.956697Z",
     "start_time": "2024-05-22T18:48:28.906739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train_lm = train_df.groupby(['capture_site', 'week_caught']).size()\n",
    "x_train_lm = train_df.reset_index(name='rescue_count')\n"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame.reset_index() got an unexpected keyword argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[327], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m x_train_lm \u001B[38;5;241m=\u001B[39m train_df\u001B[38;5;241m.\u001B[39mgroupby([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcapture_site\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweek_caught\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39msize()\n\u001B[0;32m----> 2\u001B[0m x_train_lm \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrescue_count\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: DataFrame.reset_index() got an unexpected keyword argument 'name'"
     ]
    }
   ],
   "execution_count": 327
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rescue_counts = train_df.groupby(['capture_site', 'week_caught']).size().reset_index(name='rescue_count')\n",
    "train_df = pd.merge(train_df, rescue_counts, on=['capture_site', 'week_caught'], how='left')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y = train_df['rescue_count']",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df.info()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X.isna().sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming 'X' is your feature matrix and 'y' is your target variable\n",
    "# Split the data into training and testing sets (optional)\n",
    "columns_to_drop = ['rescue_id', 'capture_site', 'turtle_characteristics', 'tag_1', 'tag_2', 'lost_tags', 't_number', 'sex', 'status', 'rescue_count', 'foraging_ground', 'capture_method', 'landing_site', 'status', 'release_site', 'week_release', 'year_release', 'weight_kg', 'ccl_cm', 'ccw_cm']\n",
    "\n",
    "X = train_df.drop(columns=columns_to_drop)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Optionally, make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Optionally, evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "\n",
    "# Compute regression metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared (R2):\", r2)\n",
    "print(\"F1 Score:\", f1)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
